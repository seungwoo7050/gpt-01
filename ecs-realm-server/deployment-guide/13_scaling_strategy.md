# 13. Scaling Strategy - ìŠ¤ì¼€ì¼ë§ ì „ëµ ê°€ì´ë“œ

## ğŸ¯ ëª©í‘œ
íŠ¸ë˜í”½ ì¦ê°€ì— ë”°ë¼ ê²Œì„ ì„œë²„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í™•ì¥í•˜ê³ , ë¹„ìš©ì„ ìµœì í™”í•˜ëŠ” ìŠ¤ì¼€ì¼ë§ ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.

## ğŸ“‹ Prerequisites
- Auto Scaling ë„êµ¬ (HPA, VPA, Cluster Autoscaler)
- ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ
- ë¶€í•˜ ì˜ˆì¸¡ ëª¨ë¸
- ë¹„ìš© ëª¨ë‹ˆí„°ë§
- ìºì‹± ì¸í”„ë¼

---

## 1. ìŠ¤ì¼€ì¼ë§ ì „ëµ ê°œìš”

### 1.1 ìŠ¤ì¼€ì¼ë§ ì°¨ì›
```yaml
scaling_dimensions:
  horizontal:
    description: "ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ì¦ê°€"
    use_cases:
      - "ë™ì‹œ ì ‘ì†ì ì¦ê°€"
      - "ì§€ì—­ë³„ ë¶„ì‚°"
      - "ê³ ê°€ìš©ì„±"
    tools:
      - "Kubernetes HPA"
      - "AWS Auto Scaling"
      - "Load Balancer"
  
  vertical:
    description: "ì¸ìŠ¤í„´ìŠ¤ í¬ê¸° ì¦ê°€"
    use_cases:
      - "CPU/ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—…"
      - "ë‹¨ì¼ ì„¸ì…˜ ì„±ëŠ¥"
      - "ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥"
    tools:
      - "Kubernetes VPA"
      - "Instance resizing"
  
  functional:
    description: "ê¸°ëŠ¥ë³„ ë¶„ë¦¬"
    use_cases:
      - "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤"
      - "íŠ¹ì • ê¸°ëŠ¥ ë¶€í•˜"
      - "ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§"
    tools:
      - "Service mesh"
      - "API Gateway"
```

### 1.2 ìŠ¤ì¼€ì¼ë§ ë©”íŠ¸ë¦­
```yaml
scaling_metrics:
  primary:
    - cpu_utilization: 70%
    - memory_utilization: 80%
    - request_rate: 1000/s
    - concurrent_users: 5000
  
  custom:
    - game_tick_latency: 50ms
    - matchmaking_queue_size: 100
    - combat_events_per_second: 10000
    - world_instance_load: 80%
```

---

## 2. Horizontal Pod Autoscaling (HPA)

### 2.1 HPA ì„¤ì •
**k8s/hpa.yaml**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: game-server-hpa
  namespace: game-production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: game-server
  
  minReplicas: 3
  maxReplicas: 100
  
  metrics:
  # CPU ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  
  # ë©”ëª¨ë¦¬ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ - ë™ì‹œ ì ‘ì†ì
  - type: Pods
    pods:
      metric:
        name: concurrent_players
      target:
        type: AverageValue
        averageValue: "1000"
  
  # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ - ìš”ì²­ ì²˜ë¦¬ìœ¨
  - type: Object
    object:
      metric:
        name: requests_per_second
      describedObject:
        apiVersion: v1
        kind: Service
        name: game-server-service
      target:
        type: Value
        value: "10000"
  
  # ì™¸ë¶€ ë©”íŠ¸ë¦­ - í í¬ê¸°
  - type: External
    external:
      metric:
        name: matchmaking_queue_size
        selector:
          matchLabels:
            queue: ranked
      target:
        type: Value
        value: "100"
  
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Min
    
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
      - type: Pods
        value: 10
        periodSeconds: 60
      selectPolicy: Max
```

### 2.2 ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì œê³µì
**src/metrics/custom_metrics.cpp**
```cpp
class CustomMetricsProvider {
private:
    prometheus::Registry registry_;
    
public:
    void RegisterMetrics() {
        // ë™ì‹œ ì ‘ì†ì ë©”íŠ¸ë¦­
        auto& concurrent_players = prometheus::BuildGauge()
            .Name("game_concurrent_players")
            .Help("Number of concurrent players")
            .Register(registry_);
        
        // ë§¤ì¹˜ë©”ì´í‚¹ í í¬ê¸°
        auto& queue_size = prometheus::BuildGauge()
            .Name("matchmaking_queue_size")
            .Help("Size of matchmaking queue")
            .Labels({{"queue", "ranked"}})
            .Register(registry_);
        
        // ì›”ë“œ ì¸ìŠ¤í„´ìŠ¤ ë¶€í•˜
        auto& world_load = prometheus::BuildGauge()
            .Name("world_instance_load")
            .Help("Load percentage of world instance")
            .Labels({{"instance", ""}})
            .Register(registry_);
    }
    
    void UpdateMetrics() {
        // ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        concurrent_players.Set(GetActivePlayers());
        queue_size.Set(GetQueueSize("ranked"));
        
        for (auto& instance : GetWorldInstances()) {
            world_load.Labels({{"instance", instance.id}})
                      .Set(instance.GetLoadPercentage());
        }
    }
};
```

---

## 3. Vertical Pod Autoscaling (VPA)

### 3.1 VPA ì„¤ì •
**k8s/vpa.yaml**
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: game-server-vpa
  namespace: game-production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: game-server
  
  updatePolicy:
    updateMode: "Auto"  # Off, Initial, Auto
  
  resourcePolicy:
    containerPolicies:
    - containerName: game-server
      minAllowed:
        cpu: 500m
        memory: 1Gi
      maxAllowed:
        cpu: 4000m
        memory: 8Gi
      controlledResources: ["cpu", "memory"]
      mode: Auto
    
    - containerName: sidecar-proxy
      mode: "Off"  # ì‚¬ì´ë“œì¹´ëŠ” ìŠ¤ì¼€ì¼ë§ ì œì™¸
```

### 3.2 ë¦¬ì†ŒìŠ¤ ê¶Œì¥ ì‚¬í•­ ë¶„ì„
```bash
#!/bin/bash
# analyze-vpa-recommendations.sh

# VPA ê¶Œì¥ì‚¬í•­ í™•ì¸
kubectl get vpa game-server-vpa -n game-production -o json | jq '.status.recommendation'

# í˜„ì¬ vs ê¶Œì¥ ë¹„êµ
echo "Current resources:"
kubectl get deployment game-server -n game-production -o json | \
    jq '.spec.template.spec.containers[0].resources'

echo "Recommended resources:"
kubectl get vpa game-server-vpa -n game-production -o json | \
    jq '.status.recommendation.containerRecommendations[0]'

# ë¹„ìš© ì˜í–¥ ê³„ì‚°
python3 calculate_cost_impact.py
```

---

## 4. Cluster Autoscaling

### 4.1 Cluster Autoscaler ì„¤ì •
**k8s/cluster-autoscaler.yaml**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  template:
    spec:
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.27.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/game-cluster
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
        - --scale-down-enabled=true
        - --scale-down-delay-after-add=10m
        - --scale-down-unneeded-time=10m
        - --scale-down-utilization-threshold=0.5
        - --max-node-provision-time=15m
        - --max-nodes-total=100
        env:
        - name: AWS_REGION
          value: us-east-1
```

### 4.2 ë…¸ë“œ ê·¸ë£¹ ì„¤ì •
```yaml
# eks-nodegroup.yaml
apiVersion: eks.amazonaws.com/v1alpha1
kind: Nodegroup
metadata:
  name: game-server-nodes
spec:
  clusterName: game-cluster
  
  scalingConfig:
    minSize: 3
    maxSize: 100
    desiredSize: 5
  
  instanceTypes:
    - c5.2xlarge
    - c5a.2xlarge  # ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•œ AMD ì¸ìŠ¤í„´ìŠ¤
    - c5n.2xlarge  # ë„¤íŠ¸ì›Œí¬ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
  
  capacityType: MIXED  # ON_DEMAND + SPOT
  
  mixedInstancesPolicy:
    instancesDistribution:
      onDemandBaseCapacity: 3
      onDemandPercentageAboveBaseCapacity: 30
      spotAllocationStrategy: capacity-optimized
  
  labels:
    workload: game-server
    scaling: auto
  
  taints:
    - key: game-server
      value: "true"
      effect: NoSchedule
  
  tags:
    Environment: production
    Team: platform
    "k8s.io/cluster-autoscaler/enabled": "true"
    "k8s.io/cluster-autoscaler/game-cluster": "owned"
```

---

## 5. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤ì¼€ì¼ë§

### 5.1 Read Replica ìë™ ìŠ¤ì¼€ì¼ë§
```python
#!/usr/bin/env python3
# db-autoscaling.py

import boto3
import time
from datetime import datetime, timedelta

class DatabaseAutoscaler:
    def __init__(self):
        self.rds = boto3.client('rds')
        self.cloudwatch = boto3.client('cloudwatch')
        self.cluster_id = 'game-aurora-cluster'
        
    def get_metrics(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        response = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/RDS',
            MetricName='DatabaseConnections',
            Dimensions=[
                {'Name': 'DBClusterIdentifier', 'Value': self.cluster_id}
            ],
            StartTime=datetime.now() - timedelta(minutes=5),
            EndTime=datetime.now(),
            Period=300,
            Statistics=['Average']
        )
        
        if response['Datapoints']:
            return response['Datapoints'][-1]['Average']
        return 0
    
    def get_read_replica_count(self):
        """í˜„ì¬ Read Replica ìˆ˜ í™•ì¸"""
        response = self.rds.describe_db_clusters(
            DBClusterIdentifier=self.cluster_id
        )
        return len(response['DBClusters'][0]['DBClusterMembers']) - 1
    
    def add_read_replica(self):
        """Read Replica ì¶”ê°€"""
        replica_id = f"{self.cluster_id}-replica-{int(time.time())}"
        
        self.rds.create_db_instance(
            DBInstanceIdentifier=replica_id,
            DBClusterIdentifier=self.cluster_id,
            DBInstanceClass='db.r5.xlarge',
            Engine='aurora-mysql',
            PubliclyAccessible=False
        )
        
        print(f"Added read replica: {replica_id}")
        
    def remove_read_replica(self, replica_id):
        """Read Replica ì œê±°"""
        self.rds.delete_db_instance(
            DBInstanceIdentifier=replica_id,
            SkipFinalSnapshot=True
        )
        
        print(f"Removed read replica: {replica_id}")
    
    def scale(self):
        """ìë™ ìŠ¤ì¼€ì¼ë§ ë¡œì§"""
        connections = self.get_metrics()
        current_replicas = self.get_read_replica_count()
        
        # ìŠ¤ì¼€ì¼ ì—… ì¡°ê±´
        if connections > 1000 and current_replicas < 5:
            self.add_read_replica()
            
        # ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì¡°ê±´
        elif connections < 500 and current_replicas > 1:
            # ê°€ì¥ ì˜¤ë˜ëœ replica ì œê±°
            replicas = self.get_replica_list()
            if replicas:
                self.remove_read_replica(replicas[0])

if __name__ == "__main__":
    scaler = DatabaseAutoscaler()
    
    while True:
        try:
            scaler.scale()
        except Exception as e:
            print(f"Error: {e}")
        
        time.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì²´í¬
```

### 5.2 ìƒ¤ë”© ì „ëµ
```sql
-- Sharding by player_id
CREATE TABLE players_shard_0 LIKE players;
CREATE TABLE players_shard_1 LIKE players;
CREATE TABLE players_shard_2 LIKE players;
CREATE TABLE players_shard_3 LIKE players;

-- ìƒ¤ë“œ ë¼ìš°íŒ… í•¨ìˆ˜
DELIMITER $$
CREATE FUNCTION get_shard_id(player_id BIGINT)
RETURNS INT
DETERMINISTIC
BEGIN
    RETURN player_id % 4;
END$$
DELIMITER ;

-- ìƒ¤ë“œë³„ ë°ì´í„° ë¶„ë°°
INSERT INTO players_shard_0 
SELECT * FROM players WHERE player_id % 4 = 0;

INSERT INTO players_shard_1 
SELECT * FROM players WHERE player_id % 4 = 1;

-- ì´í•˜ ìƒëµ...
```

---

## 6. ìºì‹± ìŠ¤ì¼€ì¼ë§

### 6.1 Redis Cluster ìë™ ìŠ¤ì¼€ì¼ë§
```bash
#!/bin/bash
# redis-autoscaling.sh

# í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í™•ì¸
MEMORY_USAGE=$(redis-cli INFO memory | grep used_memory_rss_human | cut -d: -f2 | tr -d 'M')
MEMORY_LIMIT=4096  # 4GB

if [ ${MEMORY_USAGE%.*} -gt $((MEMORY_LIMIT * 80 / 100)) ]; then
    echo "Memory usage high, adding Redis node..."
    
    # ìƒˆ ë…¸ë“œ ì¶”ê°€
    redis-cli --cluster add-node new-node:6379 existing-node:6379
    
    # ë¦¬ìƒ¤ë”©
    redis-cli --cluster reshard existing-node:6379 \
        --cluster-from all \
        --cluster-to new-node-id \
        --cluster-slots 1024 \
        --cluster-yes
fi
```

### 6.2 ìºì‹œ ì›Œë° ì „ëµ
```cpp
class CacheWarmer {
private:
    RedisClient redis_;
    DatabaseClient db_;
    
public:
    void WarmCache() {
        // 1. ìì£¼ ì ‘ê·¼í•˜ëŠ” ë°ì´í„° í”„ë¦¬ë¡œë“œ
        PreloadPopularItems();
        PreloadActivePlayerData();
        PreloadLeaderboards();
        
        // 2. ì˜ˆì¸¡ ê¸°ë°˜ ìºì‹±
        PredictiveCache();
    }
    
    void PreloadPopularItems() {
        auto popular_items = db_.Query(
            "SELECT * FROM items "
            "WHERE access_count > 1000 "
            "ORDER BY access_count DESC "
            "LIMIT 100"
        );
        
        for (const auto& item : popular_items) {
            redis_.Set("item:" + item.id, item.Serialize(), 3600);
        }
    }
    
    void PredictiveCache() {
        // ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡ ëª¨ë¸ ê¸°ë°˜ ìºì‹±
        auto current_hour = GetCurrentHour();
        auto predicted_players = GetPredictedActiveUsers(current_hour);
        
        for (const auto& player_id : predicted_players) {
            auto player_data = db_.GetPlayerData(player_id);
            redis_.Set("player:" + player_id, player_data, 1800);
        }
    }
};
```

---

## 7. ì§€ì—­ë³„ ìŠ¤ì¼€ì¼ë§

### 7.1 Multi-Region ë°°í¬
```yaml
# multi-region-deployment.yaml
regions:
  us-east-1:
    name: "North America East"
    capacity:
      min: 10
      max: 50
    latency_target: 20ms
    
  eu-west-1:
    name: "Europe West"
    capacity:
      min: 8
      max: 40
    latency_target: 25ms
    
  ap-northeast-1:
    name: "Asia Pacific"
    capacity:
      min: 15
      max: 60
    latency_target: 30ms

traffic_routing:
  method: "geoproximity"
  health_check:
    interval: 30s
    timeout: 5s
  failover:
    enabled: true
    priority:
      - us-east-1
      - eu-west-1
      - ap-northeast-1
```

### 7.2 GeoDNS ì„¤ì •
```python
# geodns-routing.py
import boto3

route53 = boto3.client('route53')

def create_geolocation_routing():
    """ì§€ì—­ë³„ ë¼ìš°íŒ… ì„¤ì •"""
    
    # ë¶ë¯¸ ë¼ìš°íŒ…
    route53.change_resource_record_sets(
        HostedZoneId='Z123456789',
        ChangeBatch={
            'Changes': [{
                'Action': 'CREATE',
                'ResourceRecordSet': {
                    'Name': 'game.example.com',
                    'Type': 'A',
                    'SetIdentifier': 'North America',
                    'GeoLocation': {
                        'ContinentCode': 'NA'
                    },
                    'AliasTarget': {
                        'HostedZoneId': 'Z215JYRZR8TBV5',
                        'DNSName': 'us-east-1-alb.amazonaws.com',
                        'EvaluateTargetHealth': True
                    }
                }
            }]
        }
    )
    
    # ìœ ëŸ½ ë¼ìš°íŒ…
    # ... (similar for EU)
    
    # ì•„ì‹œì•„ ë¼ìš°íŒ…
    # ... (similar for APAC)
```

---

## 8. ë¹„ìš© ìµœì í™”

### 8.1 Spot Instance í™œìš©
```yaml
# spot-instance-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spot-config
  namespace: karpenter
data:
  # Spot ì¸ìŠ¤í„´ìŠ¤ ë¹„ìœ¨
  spot_ratio: "70"
  
  # ì¸ìŠ¤í„´ìŠ¤ ë‹¤ì–‘ì„±
  instance_types: |
    c5.xlarge
    c5a.xlarge
    c5n.xlarge
    m5.xlarge
    m5a.xlarge
  
  # ì¸í„°ëŸ½ì…˜ ì²˜ë¦¬
  interruption_handler: |
    #!/bin/bash
    # 2ë¶„ ê²½ê³  ì‹œ graceful shutdown
    kubectl drain $NODE_NAME --ignore-daemonsets --delete-emptydir-data
    kubectl cordon $NODE_NAME
```

### 8.2 ë¹„ìš© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
```python
# cost-monitoring.py
import boto3
from datetime import datetime, timedelta

ce = boto3.client('ce')

def get_daily_costs():
    """ì¼ì¼ ë¹„ìš© ì¡°íšŒ"""
    response = ce.get_cost_and_usage(
        TimePeriod={
            'Start': (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d'),
            'End': datetime.now().strftime('%Y-%m-%d')
        },
        Granularity='DAILY',
        Metrics=['UnblendedCost'],
        GroupBy=[
            {'Type': 'DIMENSION', 'Key': 'SERVICE'},
            {'Type': 'TAG', 'Key': 'Environment'}
        ]
    )
    
    return response['ResultsByTime']

def analyze_cost_trends():
    """ë¹„ìš© íŠ¸ë Œë“œ ë¶„ì„"""
    costs = get_daily_costs()
    
    # ì„œë¹„ìŠ¤ë³„ ë¹„ìš© ì§‘ê³„
    service_costs = {}
    for day in costs:
        for group in day['Groups']:
            service = group['Keys'][0]
            cost = float(group['Metrics']['UnblendedCost']['Amount'])
            
            if service not in service_costs:
                service_costs[service] = []
            service_costs[service].append(cost)
    
    # ë¹„ìš© ì¦ê°€ ì•Œë¦¼
    for service, costs in service_costs.items():
        if len(costs) > 7:
            week_avg = sum(costs[-7:]) / 7
            prev_week_avg = sum(costs[-14:-7]) / 7
            
            if week_avg > prev_week_avg * 1.2:  # 20% ì¦ê°€
                send_alert(f"Cost increase detected for {service}: {week_avg:.2f}/day")
```

---

## 9. ì˜ˆì¸¡ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§

### 9.1 ML ê¸°ë°˜ íŠ¸ë˜í”½ ì˜ˆì¸¡
```python
# predictive-scaling.py
from sklearn.ensemble import RandomForestRegressor
import pandas as pd
import numpy as np

class PredictiveScaler:
    def __init__(self):
        self.model = RandomForestRegressor(n_estimators=100)
        self.train_model()
    
    def train_model(self):
        """ê³¼ê±° ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ"""
        # ê³¼ê±° íŠ¸ë˜í”½ ë°ì´í„° ë¡œë“œ
        data = pd.read_csv('traffic_history.csv')
        
        # íŠ¹ì§• ì¶”ì¶œ
        data['hour'] = pd.to_datetime(data['timestamp']).dt.hour
        data['day_of_week'] = pd.to_datetime(data['timestamp']).dt.dayofweek
        data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)
        data['is_event'] = data['event_active'].astype(int)
        
        # í•™ìŠµ
        features = ['hour', 'day_of_week', 'is_weekend', 'is_event']
        X = data[features]
        y = data['concurrent_users']
        
        self.model.fit(X, y)
    
    def predict_traffic(self, hours_ahead=1):
        """ë¯¸ë˜ íŠ¸ë˜í”½ ì˜ˆì¸¡"""
        future_time = datetime.now() + timedelta(hours=hours_ahead)
        
        features = {
            'hour': future_time.hour,
            'day_of_week': future_time.weekday(),
            'is_weekend': 1 if future_time.weekday() in [5, 6] else 0,
            'is_event': self.check_event_schedule(future_time)
        }
        
        predicted_users = self.model.predict([list(features.values())])[0]
        return int(predicted_users)
    
    def scale_resources(self):
        """ì˜ˆì¸¡ ê¸°ë°˜ ë¦¬ì†ŒìŠ¤ ìŠ¤ì¼€ì¼ë§"""
        # 1ì‹œê°„ í›„ íŠ¸ë˜í”½ ì˜ˆì¸¡
        predicted = self.predict_traffic(1)
        current = self.get_current_users()
        
        if predicted > current * 1.3:  # 30% ì¦ê°€ ì˜ˆìƒ
            # ë¯¸ë¦¬ ìŠ¤ì¼€ì¼ ì—…
            required_pods = math.ceil(predicted / 1000)  # 1000 users per pod
            self.scale_deployment(required_pods)
            
        elif predicted < current * 0.7:  # 30% ê°ì†Œ ì˜ˆìƒ
            # ì ì§„ì  ìŠ¤ì¼€ì¼ ë‹¤ìš´
            required_pods = math.ceil(predicted / 1000)
            self.scale_deployment(required_pods)
```

---

## 10. ìŠ¤ì¼€ì¼ë§ ëª¨ë‹ˆí„°ë§

### 10.1 ìŠ¤ì¼€ì¼ë§ ì´ë²¤íŠ¸ ì¶”ì 
```yaml
# scaling-events-dashboard.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: scaling-dashboard
data:
  queries: |
    # HPA ìŠ¤ì¼€ì¼ë§ ì´ë²¤íŠ¸
    kube_horizontalpodautoscaler_status_current_replicas{namespace="game-production"}
    
    # ë…¸ë“œ ìŠ¤ì¼€ì¼ë§ ì´ë²¤íŠ¸
    kube_node_info{}
    
    # ìŠ¤ì¼€ì¼ë§ ì§€ì—° ì‹œê°„
    histogram_quantile(0.95, 
      rate(cluster_autoscaler_scaled_up_nodes_duration_seconds_bucket[5m])
    )
    
    # ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨
    cluster_autoscaler_failed_scale_ups_total
    
    # ë¹„ìš© ì˜í–¥
    sum(kube_node_info) * 0.5  # $0.5 per hour per node
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ìŠ¤ì¼€ì¼ë§ ì„¤ì •
- [ ] HPA êµ¬ì„± ë° í…ŒìŠ¤íŠ¸
- [ ] VPA êµ¬ì„± ë° ê¶Œì¥ì‚¬í•­ ê²€í† 
- [ ] Cluster Autoscaler ì„¤ì •
- [ ] ë°ì´í„°ë² ì´ìŠ¤ Read Replica ìë™í™”
- [ ] ìºì‹œ í´ëŸ¬ìŠ¤í„° í™•ì¥ ì„¤ì •

### ìµœì í™”
- [ ] Spot Instance í™œìš©
- [ ] ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•
- [ ] ë¹„ìš© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
- [ ] ì§€ì—­ë³„ ë°°í¬ ì „ëµ
- [ ] ìºì‹œ ì›Œë° êµ¬í˜„

### ëª¨ë‹ˆí„°ë§
- [ ] ìŠ¤ì¼€ì¼ë§ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ
- [ ] ë¹„ìš© ì•Œë¦¼ ì„¤ì •
- [ ] ì„±ëŠ¥ ì„ê³„ê°’ ì •ì˜
- [ ] ìŠ¤ì¼€ì¼ë§ ì´ë²¤íŠ¸ ë¡œê¹…
- [ ] ìš©ëŸ‰ ê³„íš ë¦¬í¬íŠ¸

## ğŸ¯ ì™„ë£Œ
ë°°í¬ ê°€ì´ë“œì˜ ëª¨ë“  ë¬¸ì„œê°€ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!