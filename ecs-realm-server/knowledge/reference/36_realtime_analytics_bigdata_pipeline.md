# 22ë‹¨ê³„: ì‹¤ì‹œê°„ ë¶„ì„ & ë¹…ë°ì´í„° íŒŒì´í”„ë¼ì¸ - ê²Œì„ ë°ì´í„°ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ê¸°
*1ì´ˆë§ˆë‹¤ 100ë§Œ ê°œì˜ ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ í”Œë ˆì´ì–´ í–‰ë™ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ë§ˆë²•*

> **ğŸ¯ ëª©í‘œ**: ê²Œì„ì—ì„œ ë°œìƒí•˜ëŠ” ëŒ€ëŸ‰ì˜ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì²˜ë¦¬, ë¶„ì„í•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

---

## ğŸ“‹ ë¬¸ì„œ ì •ë³´

- **ë‚œì´ë„**: ğŸŸ¡ ì¤‘ê¸‰â†’ğŸ”´ ê³ ê¸‰ (ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ì˜ ì ˆì •!)
- **ì˜ˆìƒ í•™ìŠµ ì‹œê°„**: 12-15ì‹œê°„ (ë¹…ë°ì´í„°ëŠ” ì‹œê°„ì´ í•„ìš”!)
- **í•„ìš” ì„ ìˆ˜ ì§€ì‹**: 
  - âœ… [1-21ë‹¨ê³„](./01_advanced_cpp_features.md) ëª¨ë“  ë‚´ìš© ì™„ë£Œ
  - âœ… ê¸°ë³¸ì ì¸ í†µê³„í•™ ì§€ì‹
  - âœ… ë¹…ë°ì´í„° ê°œë… (Hadoop, Spark ë“±)
  - âœ… ê²Œì„ ìš´ì˜ ê²½í—˜ ë˜ëŠ” ê´€ì‹¬
- **ì‹¤ìŠµ í™˜ê²½**: Apache Spark, Kafka, ClickHouse, Python
- **ìµœì¢… ê²°ê³¼ë¬¼**: ë„·í”Œë¦­ìŠ¤ê¸‰ ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ!

---

## ğŸ¤” ì™œ ê²Œì„ì—ì„œ ì‹¤ì‹œê°„ ë¶„ì„ì´ ì¤‘ìš”í• ê¹Œ?

### **í˜„ëŒ€ ê²Œì„ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ í˜„ì‹¤**

```cpp
// ğŸ˜° ë°ì´í„° ì—†ì´ ìš´ì˜í•˜ëŠ” ê²Œì„ì˜ ê²°ë§
void TraditionalGameOperation() {
    // ê°ìœ¼ë¡œ ìš´ì˜
    if (user_complaints > "ë§ë‹¤ê³  ëŠë‚Œ") {
        panic_patch();  // ê¸‰í•˜ê²Œ íŒ¨ì¹˜
    }
    
    // 3ê°œì›” í›„
    if (revenue < expected) {
        std::cout << "ì™œ ë§¤ì¶œì´ ë–¨ì–´ì¡Œì„ê¹Œ?" << std::endl;
        // ì´ë¯¸ ëŠ¦ìŒ - ìœ ì €ë“¤ ì´ë¯¸ ì´íƒˆ
    }
}

// âœ… ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ìš´ì˜í•˜ëŠ” ê²Œì„
class DataDrivenGameOperation {
public:
    void MonitorRealtime() {
        // ì‹¤ì‹œê°„ ì§€í‘œ ëª¨ë‹ˆí„°ë§
        if (concurrent_users < threshold) {
            TriggerEvent("ì‚¬ìš©ì ê°ì†Œ ì´ë²¤íŠ¸");
        }
        
        if (churn_prediction_model.predict(user) > 0.7f) {
            SendRetentionCampaign(user);  // ì´íƒˆ ì˜ˆìƒ ìœ ì €ì—ê²Œ ì¦‰ì‹œ ëŒ€ì‘
        }
        
        if (revenue_anomaly_detected()) {
            AlertBusinessTeam();  // ë§¤ì¶œ ì´ìƒ ì¦‰ì‹œ ê°ì§€
        }
    }
};
```

**ë°ì´í„° ê¸°ë°˜ ìš´ì˜ì˜ íš¨ê³¼:**
- **ìœ ì € ì´íƒˆë¥ **: 30% â†’ 15% (50% ê°ì†Œ)
- **ë§¤ì¶œ ì˜ˆì¸¡ ì •í™•ë„**: 60% â†’ 85% (ì˜ì‚¬ê²°ì • í’ˆì§ˆ í–¥ìƒ)
- **ë¬¸ì œ ëŒ€ì‘ ì‹œê°„**: 3ì¼ â†’ 30ë¶„ (ì‹¤ì‹œê°„ ëŒ€ì‘)
- **A/B í…ŒìŠ¤íŠ¸ ì†ë„**: 1ê°œì›” â†’ 1ì£¼ì¼ (ë¹ ë¥¸ ì‹¤í—˜)

---

## ğŸ—ï¸ 1. ê²Œì„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜

### **1.1 ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”**

```
ğŸ“± Game Clients (5,000+ users)
    â†“ (ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì „ì†¡)
ğŸŒ API Gateway + Load Balancer
    â†“
ğŸ“¨ Message Queue (Apache Kafka)
    â†“ (ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬)
âš¡ Stream Processing (Apache Spark)
    â†“ (ì‹¤ì‹œê°„ ì§‘ê³„)
ğŸ—„ï¸ Real-time Database (ClickHouse)
    â†“ (ë°°ì¹˜ ì²˜ë¦¬)
ğŸ“Š Data Warehouse (Apache Spark + Parquet)
    â†“ (ë¨¸ì‹ ëŸ¬ë‹)
ğŸ¤– ML Pipeline (Python + MLflow)
    â†“ (ì‹œê°í™”)
ğŸ“ˆ Dashboard (Grafana + Custom WebApp)
```

### **1.2 ë°ì´í„° ë ˆì´ì–´ êµ¬ì¡°**

```cpp
#include <nlohmann/json.hpp>
#include <chrono>
#include <string>

// ğŸ® ê²Œì„ì—ì„œ ë°œìƒí•˜ëŠ” ì´ë²¤íŠ¸ íƒ€ì…ë“¤
enum class GameEventType {
    USER_LOGIN,           // ë¡œê·¸ì¸
    USER_LOGOUT,          // ë¡œê·¸ì•„ì›ƒ
    LEVEL_UP,            // ë ˆë²¨ì—…
    ITEM_PURCHASE,       // ì•„ì´í…œ êµ¬ë§¤
    SKILL_USE,           // ìŠ¤í‚¬ ì‚¬ìš©
    PLAYER_DEATH,        // í”Œë ˆì´ì–´ ì‚¬ë§
    QUEST_COMPLETE,      // í€˜ìŠ¤íŠ¸ ì™„ë£Œ
    CHAT_MESSAGE,        // ì±„íŒ…
    GUILD_ACTION,        // ê¸¸ë“œ ê´€ë ¨ í–‰ë™
    PVP_BATTLE,          // PvP ì „íˆ¬
    PAYMENT,             // ê²°ì œ
    ERROR_OCCURRED       // ì˜¤ë¥˜ ë°œìƒ
};

// ğŸ“Š í‘œì¤€í™”ëœ ê²Œì„ ì´ë²¤íŠ¸ êµ¬ì¡°
struct GameEvent {
    std::string event_id;         // ê³ ìœ  ID
    GameEventType event_type;     // ì´ë²¤íŠ¸ íƒ€ì…
    uint21_t user_id;            // ì‚¬ìš©ì ID
    std::string session_id;       // ì„¸ì…˜ ID
    uint21_t timestamp_ms;        // íƒ€ì„ìŠ¤íƒ¬í”„ (ë°€ë¦¬ì´ˆ)
    std::string server_id;        // ì„œë²„ ID
    nlohmann::json properties;    // ì´ë²¤íŠ¸ë³„ ì¶”ê°€ ë°ì´í„°
    std::string client_version;   // í´ë¼ì´ì–¸íŠ¸ ë²„ì „
    std::string platform;         // í”Œë«í¼ (iOS, Android, PC)
    
    // ìŠ¤í‚¤ë§ˆ ê²€ì¦
    bool IsValid() const {
        return !event_id.empty() && 
               user_id > 0 && 
               timestamp_ms > 0 &&
               !session_id.empty();
    }
    
    // JSON ì§ë ¬í™”
    nlohmann::json ToJson() const {
        return {
            {"event_id", event_id},
            {"event_type", static_cast<int>(event_type)},
            {"user_id", user_id},
            {"session_id", session_id},
            {"timestamp_ms", timestamp_ms},
            {"server_id", server_id},
            {"properties", properties},
            {"client_version", client_version},
            {"platform", platform}
        };
    }
    
    // JSON ì—­ì§ë ¬í™”
    static GameEvent FromJson(const nlohmann::json& j) {
        GameEvent event;
        event.event_id = j["event_id"];
        event.event_type = static_cast<GameEventType>(j["event_type"]);
        event.user_id = j["user_id"];
        event.session_id = j["session_id"];
        event.timestamp_ms = j["timestamp_ms"];
        event.server_id = j["server_id"];
        event.properties = j["properties"];
        event.client_version = j["client_version"];
        event.platform = j["platform"];
        return event;
    }
};

// ğŸ¯ íŠ¹í™”ëœ ì´ë²¤íŠ¸ ìƒì„±ê¸°ë“¤
class GameEventFactory {
public:
    static GameEvent CreateLoginEvent(uint21_t user_id, const std::string& session_id) {
        GameEvent event;
        event.event_id = GenerateUUID();
        event.event_type = GameEventType::USER_LOGIN;
        event.user_id = user_id;
        event.session_id = session_id;
        event.timestamp_ms = GetCurrentTimestampMs();
        event.properties = {
            {"login_method", "password"},
            {"device_type", "mobile"},
            {"ip_country", "KR"}
        };
        return event;
    }
    
    static GameEvent CreatePurchaseEvent(uint21_t user_id, const std::string& session_id,
                                       const std::string& item_id, int price, const std::string& currency) {
        GameEvent event;
        event.event_id = GenerateUUID();
        event.event_type = GameEventType::ITEM_PURCHASE;
        event.user_id = user_id;
        event.session_id = session_id;
        event.timestamp_ms = GetCurrentTimestampMs();
        event.properties = {
            {"item_id", item_id},
            {"price", price},
            {"currency", currency},
            {"payment_method", "credit_card"},
            {"promotion_id", ""},
            {"is_first_purchase", false}
        };
        return event;
    }
    
    static GameEvent CreateBattleEvent(uint21_t attacker_id, uint21_t defender_id,
                                     const std::string& session_id, const std::string& result) {
        GameEvent event;
        event.event_id = GenerateUUID();
        event.event_type = GameEventType::PVP_BATTLE;
        event.user_id = attacker_id;
        event.session_id = session_id;
        event.timestamp_ms = GetCurrentTimestampMs();
        event.properties = {
            {"defender_id", defender_id},
            {"battle_result", result},  // "win", "lose", "draw"
            {"battle_duration_sec", 45},
            {"damage_dealt", 1200},
            {"damage_received", 800},
            {"location", "arena_1"}
        };
        return event;
    }

private:
    static std::string GenerateUUID() {
        // UUID ìƒì„± ë¡œì§ (ì‹¤ì œë¡œëŠ” uuid ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
        return "evt_" + std::to_string(GetCurrentTimestampMs()) + "_" + 
               std::to_string(rand() % 10000);
    }
    
    static uint21_t GetCurrentTimestampMs() {
        return std::chrono::duration_cast<std::chrono::milliseconds>(
            std::chrono::system_clock::now().time_since_epoch()).count();
    }
};
```

---

## âš¡ 2. Apache Spark ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„

### **2.1 ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì—”ì§„**

```cpp
// ğŸ”¥ ê³ ì„±ëŠ¥ ì´ë²¤íŠ¸ í”„ë¡œì„¸ì„œ (C++ â†’ Python Spark ì—°ë™)
#include <kafka/kafka.h>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>

class GameEventStreamProcessor {
private:
    kafka::Consumer consumer_;
    std::atomic<bool> running_{true};
    std::thread processing_thread_;
    
    // ì‹¤ì‹œê°„ ì§‘ê³„ ë°ì´í„°
    struct RealTimeMetrics {
        std::atomic<uint21_t> concurrent_users{0};
        std::atomic<uint21_t> events_per_second{0};
        std::atomic<double> average_session_length{0};
        std::atomic<uint21_t> total_revenue_today{0};
        
        std::unordered_map<std::string, std::atomic<uint21_t>> events_by_type;
        std::unordered_map<std::string, std::atomic<uint21_t>> users_by_platform;
        
        mutable std::mutex metrics_mutex;
    };
    
    RealTimeMetrics metrics_;
    
public:
    GameEventStreamProcessor(const std::string& kafka_brokers, const std::string& topic) 
        : consumer_(kafka::Consumer::create({
            {"bootstrap.servers", kafka_brokers},
            {"group.id", "game_analytics_processor"},
            {"auto.offset.reset", "latest"},
            {"enable.auto.commit", "true"}
        })) {
        
        consumer_.subscribe({topic});
        
        // ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        processing_thread_ = std::thread(&GameEventStreamProcessor::ProcessEvents, this);
    }
    
    ~GameEventStreamProcessor() {
        Stop();
    }
    
    void Stop() {
        running_ = false;
        if (processing_thread_.joinable()) {
            processing_thread_.join();
        }
    }
    
    // ğŸ“Š ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    void ProcessEvents() {
        const std::chrono::seconds timeout(1);
        
        while (running_) {
            auto messages = consumer_.poll(timeout);
            
            for (auto& message : messages) {
                if (message.get_error()) {
                    std::cerr << "Kafka error: " << message.get_error() << std::endl;
                    continue;
                }
                
                try {
                    // JSON íŒŒì‹±
                    auto json_data = nlohmann::json::parse(message.get_payload());
                    auto event = GameEvent::FromJson(json_data);
                    
                    if (!event.IsValid()) {
                        std::cerr << "Invalid event: " << event.event_id << std::endl;
                        continue;
                    }
                    
                    // ì‹¤ì‹œê°„ ì§‘ê³„ ì—…ë°ì´íŠ¸
                    UpdateRealTimeMetrics(event);
                    
                    // íŠ¹ë³„í•œ ì²˜ë¦¬ê°€ í•„ìš”í•œ ì´ë²¤íŠ¸ë“¤
                    ProcessSpecialEvents(event);
                    
                } catch (const std::exception& e) {
                    std::cerr << "Event processing error: " << e.what() << std::endl;
                }
            }
            
            // ë§¤ ì´ˆë§ˆë‹¤ ë©”íŠ¸ë¦­ì„ ClickHouseì— ì „ì†¡
            static auto last_flush = std::chrono::steady_clock::now();
            auto now = std::chrono::steady_clock::now();
            if (now - last_flush >= std::chrono::seconds(1)) {
                FlushMetricsToClickHouse();
                last_flush = now;
            }
        }
    }
    
private:
    void UpdateRealTimeMetrics(const GameEvent& event) {
        metrics_.events_per_second++;
        
        // ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì¹´ìš´íŒ…
        std::string event_type_str = std::to_string(static_cast<int>(event.event_type));
        metrics_.events_by_type[event_type_str]++;
        
        // í”Œë«í¼ë³„ ì‚¬ìš©ì ì¹´ìš´íŒ…
        metrics_.users_by_platform[event.platform]++;
        
        // íŠ¹ë³„í•œ ì´ë²¤íŠ¸ ì²˜ë¦¬
        switch (event.event_type) {
            case GameEventType::USER_LOGIN:
                metrics_.concurrent_users++;
                break;
                
            case GameEventType::USER_LOGOUT:
                if (metrics_.concurrent_users > 0) {
                    metrics_.concurrent_users--;
                }
                break;
                
            case GameEventType::ITEM_PURCHASE:
                if (event.properties.contains("price")) {
                    int price = event.properties["price"];
                    metrics_.total_revenue_today += price;
                }
                break;
        }
    }
    
    void ProcessSpecialEvents(const GameEvent& event) {
        // ğŸš¨ ì´ìƒ ì§•í›„ íƒì§€
        if (event.event_type == GameEventType::ERROR_OCCURRED) {
            std::string error_type = event.properties.value("error_type", "unknown");
            
            // í¬ë¦¬í‹°ì»¬ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì•Œë¦¼
            if (error_type == "crash" || error_type == "memory_leak") {
                SendAlertToDevTeam(event);
            }
        }
        
        // ğŸ’° ê³ ì•¡ ê²°ì œ ëª¨ë‹ˆí„°ë§
        if (event.event_type == GameEventType::PAYMENT) {
            int amount = event.properties.value("amount", 0);
            if (amount > 10000) {  // 100ë‹¬ëŸ¬ ì´ìƒ
                LogHighValueTransaction(event);
            }
        }
        
        // ğŸ¯ íŠ¹ì´í•œ í–‰ë™ íŒ¨í„´ ê°ì§€
        if (event.event_type == GameEventType::SKILL_USE) {
            std::string skill_id = event.properties.value("skill_id", "");
            DetectSuspiciousActivity(event.user_id, skill_id, event.timestamp_ms);
        }
    }
    
    void FlushMetricsToClickHouse() {
        // ClickHouseì— ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì „ì†¡
        std::lock_guard<std::mutex> lock(metrics_.metrics_mutex);
        
        auto now = std::chrono::system_clock::now();
        auto timestamp = std::chrono::duration_cast<std::chrono::seconds>(
            now.time_since_epoch()).count();
        
        std::string query = fmt::format(
            "INSERT INTO realtime_metrics "
            "(timestamp, concurrent_users, events_per_second, total_revenue_today) "
            "VALUES ({}, {}, {}, {})",
            timestamp,
            metrics_.concurrent_users.load(),
            metrics_.events_per_second.exchange(0),  // ë¦¬ì…‹í•˜ë©´ì„œ ì½ê¸°
            metrics_.total_revenue_today.load()
        );
        
        // ClickHouse í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡
        ExecuteClickHouseQuery(query);
    }
    
    void SendAlertToDevTeam(const GameEvent& event) {
        // Slack, PagerDuty ë“±ìœ¼ë¡œ ì¦‰ì‹œ ì•Œë¦¼
        std::cout << "ğŸš¨ CRITICAL ALERT: " << event.ToJson().dump() << std::endl;
    }
    
    void LogHighValueTransaction(const GameEvent& event) {
        // ê³ ì•¡ ê²°ì œ ë¡œê¹… (ì‚¬ê¸° ë°©ì§€)
        std::cout << "ğŸ’° High Value Transaction: " << event.ToJson().dump() << std::endl;
    }
    
    void DetectSuspiciousActivity(uint21_t user_id, const std::string& skill_id, uint21_t timestamp) {
        // ë§¤í¬ë¡œ ì‚¬ìš© ì˜ì‹¬ í–‰ë™ ê°ì§€
        static std::unordered_map<uint21_t, std::vector<uint21_t>> user_skill_timings;
        
        auto& timings = user_skill_timings[user_id];
        timings.push_back(timestamp);
        
        // ìµœê·¼ 10ì´ˆê°„ì˜ ìŠ¤í‚¬ ì‚¬ìš©ë§Œ ìœ ì§€
        auto cutoff = timestamp - 10000;  // 10ì´ˆ ì „
        timings.erase(
            std::remove_if(timings.begin(), timings.end(),
                [cutoff](uint21_t t) { return t < cutoff; }),
            timings.end()
        );
        
        // 10ì´ˆì— 50ë²ˆ ì´ìƒ ìŠ¤í‚¬ ì‚¬ìš© ì‹œ ì˜ì‹¬
        if (timings.size() >= 50) {
            std::cout << "ğŸ¤– Suspicious Activity Detected: User " << user_id 
                      << " used skills " << timings.size() << " times in 10 seconds" << std::endl;
        }
    }
    
    void ExecuteClickHouseQuery(const std::string& query) {
        // ì‹¤ì œ ClickHouse í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
        // clickhouse-cpp ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
        std::cout << "ClickHouse Query: " << query << std::endl;
    }
};
```

### **2.2 Spark Streaming Python ì½”ë“œ**

```python
# ğŸ Apache Sparkë¡œ ëŒ€ê·œëª¨ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import json

class GameAnalyticsSparkProcessor:
    def __init__(self):
        self.spark = SparkSession.builder \
            .appName("GameAnalyticsProcessor") \
            .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoints") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("WARN")
    
    def process_game_events(self):
        """ê²Œì„ ì´ë²¤íŠ¸ ì‹¤ì‹œê°„ ì²˜ë¦¬"""
        
        # Kafkaì—ì„œ ìŠ¤íŠ¸ë¦¼ ì½ê¸°
        raw_stream = self.spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "game_events") \
            .option("startingOffsets", "latest") \
            .load()
        
        # ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ì •ì˜
        event_schema = StructType([
            StructField("event_id", StringType(), True),
            StructField("event_type", IntegerType(), True),
            StructField("user_id", LongType(), True),
            StructField("session_id", StringType(), True),
            StructField("timestamp_ms", LongType(), True),
            StructField("server_id", StringType(), True),
            StructField("properties", MapType(StringType(), StringType()), True),
            StructField("client_version", StringType(), True),
            StructField("platform", StringType(), True)
        ])
        
        # JSON íŒŒì‹± ë° ë³€í™˜
        parsed_events = raw_stream \
            .select(from_json(col("value").cast("string"), event_schema).alias("event")) \
            .select("event.*") \
            .withColumn("timestamp", to_timestamp(col("timestamp_ms") / 1000)) \
            .withColumn("hour", hour("timestamp")) \
            .withColumn("date", to_date("timestamp"))
        
        # ğŸ¯ ì‹¤ì‹œê°„ ì§‘ê³„ 1: ì‹œê°„ë‹¹ í™œì„± ì‚¬ìš©ì ìˆ˜ (DAU/MAU ê³„ì‚°)
        hourly_active_users = parsed_events \
            .withWatermark("timestamp", "1 minute") \
            .groupBy(
                window("timestamp", "1 hour"),
                "platform"
            ) \
            .agg(
                countDistinct("user_id").alias("active_users"),
                count("*").alias("total_events")
            ) \
            .writeStream \
            .outputMode("update") \
            .format("console") \
            .trigger(processingTime="30 seconds") \
            .start()
        
        # ğŸ¯ ì‹¤ì‹œê°„ ì§‘ê³„ 2: ë§¤ì¶œ ëª¨ë‹ˆí„°ë§
        revenue_stream = parsed_events \
            .filter(col("event_type") == 4)  # ITEM_PURCHASE \
            .select(
                "timestamp",
                "user_id",
                get_json_object("properties", "$.price").cast("double").alias("price"),
                get_json_object("properties", "$.currency").alias("currency"),
                get_json_object("properties", "$.item_id").alias("item_id")
            ) \
            .withWatermark("timestamp", "1 minute") \
            .groupBy(
                window("timestamp", "5 minutes"),
                "currency"
            ) \
            .agg(
                sum("price").alias("total_revenue"),
                count("*").alias("transaction_count"),
                countDistinct("user_id").alias("paying_users")
            ) \
            .writeStream \
            .outputMode("update") \
            .format("console") \
            .trigger(processingTime="10 seconds") \
            .start()
        
        # ğŸ¯ ì‹¤ì‹œê°„ ì§‘ê³„ 3: ê²Œì„í”Œë ˆì´ íŒ¨í„´ ë¶„ì„
        gameplay_analysis = parsed_events \
            .filter(col("event_type").isin([2, 3, 5, 6]))  # LEVEL_UP, SKILL_USE, PLAYER_DEATH, QUEST_COMPLETE \
            .withWatermark("timestamp", "2 minutes") \
            .groupBy(
                window("timestamp", "10 minutes"),
                "event_type",
                get_json_object("properties", "$.level").cast("int").alias("player_level")
            ) \
            .agg(
                count("*").alias("event_count"),
                countDistinct("user_id").alias("unique_players")
            ) \
            .writeStream \
            .outputMode("update") \
            .format("console") \
            .trigger(processingTime="30 seconds") \
            .start()
        
        # ğŸš¨ ì‹¤ì‹œê°„ ì´ìƒ íƒì§€: ê¸‰ê²©í•œ ì´íƒˆë¥  ì¦ê°€
        churn_detection = parsed_events \
            .filter(col("event_type") == 1)  # USER_LOGOUT \
            .withWatermark("timestamp", "1 minute") \
            .groupBy(window("timestamp", "5 minutes")) \
            .agg(
                count("*").alias("logout_count"),
                countDistinct("user_id").alias("users_logged_out")
            ) \
            .withColumn("logout_rate", col("logout_count") / col("users_logged_out")) \
            .filter(col("logout_rate") > 2.0)  # í‰ê· ë³´ë‹¤ 2ë°° ë†’ì€ ì´íƒˆë¥  \
            .writeStream \
            .outputMode("update") \
            .foreachBatch(self.send_churn_alert) \
            .trigger(processingTime="1 minute") \
            .start()
        
        return [hourly_active_users, revenue_stream, gameplay_analysis, churn_detection]
    
    def send_churn_alert(self, batch_df, batch_id):
        """ì´íƒˆë¥  ê¸‰ì¦ ì•Œë¦¼"""
        if batch_df.count() > 0:
            print(f"ğŸš¨ CHURN ALERT - Batch {batch_id}:")
            batch_df.show(truncate=False)
            
            # ì‹¤ì œë¡œëŠ” Slack, PagerDuty ë“±ìœ¼ë¡œ ì•Œë¦¼ ë°œì†¡
            # self.send_slack_notification(batch_df)
    
    def create_real_time_dashboard_data(self):
        """ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œìš© ë°ì´í„° ìƒì„±"""
        
        # Kafkaì—ì„œ ì½ê¸°
        stream = self.spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "game_events") \
            .load()
        
        # ëŒ€ì‹œë³´ë“œìš© ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ê³„ì‚°
        dashboard_metrics = stream \
            .select(from_json(col("value").cast("string"), self.get_event_schema()).alias("event")) \
            .select("event.*") \
            .withColumn("timestamp", to_timestamp(col("timestamp_ms") / 1000)) \
            .withWatermark("timestamp", "30 seconds") \
            .groupBy(window("timestamp", "1 minute")) \
            .agg(
                count("*").alias("events_per_minute"),
                countDistinct("user_id").alias("concurrent_users"),
                sum(when(col("event_type") == 4, 
                         get_json_object("properties", "$.price").cast("double"))
                    .otherwise(0)).alias("revenue_per_minute"),
                countDistinct(when(col("event_type") == 0, col("user_id"))).alias("new_logins"),
                countDistinct(when(col("event_type") == 1, col("user_id"))).alias("logouts")
            ) \
            .select(
                col("window.start").alias("timestamp"),
                "*"
            ) \
            .drop("window")
        
        # ClickHouseì— ì‹¤ì‹œê°„ ì €ì¥
        query = dashboard_metrics \
            .writeStream \
            .outputMode("update") \
            .format("jdbc") \
            .option("url", "jdbc:clickhouse://localhost:8123/analytics") \
            .option("dbtable", "real_time_dashboard") \
            .option("user", "default") \
            .option("driver", "ru.yandex.clickhouse.ClickHouseDriver") \
            .trigger(processingTime="10 seconds") \
            .start()
        
        return query
    
    def get_event_schema(self):
        """ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ë°˜í™˜"""
        return StructType([
            StructField("event_id", StringType(), True),
            StructField("event_type", IntegerType(), True),
            StructField("user_id", LongType(), True),
            StructField("session_id", StringType(), True),
            StructField("timestamp_ms", LongType(), True),
            StructField("server_id", StringType(), True),
            StructField("properties", MapType(StringType(), StringType()), True),
            StructField("client_version", StringType(), True),
            StructField("platform", StringType(), True)
        ])

# ğŸš€ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    processor = GameAnalyticsSparkProcessor()
    
    # ëª¨ë“  ìŠ¤íŠ¸ë¦¼ ì‹œì‘
    streams = processor.process_game_events()
    dashboard_stream = processor.create_real_time_dashboard_data()
    
    # ëª¨ë“  ìŠ¤íŠ¸ë¦¼ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    for stream in streams + [dashboard_stream]:
        stream.awaitTermination()
```

---

## ğŸ—„ï¸ 3. ClickHouse ì‹¤ì‹œê°„ ë¶„ì„ ë°ì´í„°ë² ì´ìŠ¤

### **3.1 ClickHouse ìŠ¤í‚¤ë§ˆ ì„¤ê³„**

```sql
-- ğŸ“Š ê²Œì„ ì´ë²¤íŠ¸ ì›ë³¸ ë°ì´í„° í…Œì´ë¸” (MergeTree ì—”ì§„)
CREATE TABLE game_events (
    event_id String,
    event_type UInt8,
    user_id UInt64,
    session_id String,
    timestamp DateTime64(3),
    server_id String,
    properties String,  -- JSON í˜•íƒœë¡œ ì €ì¥
    client_version String,
    platform LowCardinality(String),  -- ì¹´ë””ë„ë¦¬í‹° ìµœì í™”
    date Date MATERIALIZED toDate(timestamp)  -- íŒŒí‹°ì…”ë‹ìš©
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)  -- ì›”ë³„ íŒŒí‹°ì…”ë‹
ORDER BY (event_type, user_id, timestamp)
TTL timestamp + INTERVAL 1 YEAR DELETE;  -- 1ë…„ í›„ ìë™ ì‚­ì œ

-- ğŸ“ˆ ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì§‘ê³„ í…Œì´ë¸”
CREATE TABLE realtime_metrics (
    timestamp DateTime,
    concurrent_users UInt32,
    events_per_second UInt32,
    total_revenue_today UInt64,
    platform LowCardinality(String),
    server_id String
) ENGINE = MergeTree()
ORDER BY (timestamp, platform, server_id)
TTL timestamp + INTERVAL 30 DAY DELETE;

-- ğŸ’° ë§¤ì¶œ ë¶„ì„ìš© ì§‘ê³„ í…Œì´ë¸”
CREATE TABLE revenue_analytics (
    date Date,
    hour UInt8,
    platform LowCardinality(String),
    currency LowCardinality(String),
    total_revenue Decimal64(2),
    transaction_count UInt32,
    paying_users UInt32,
    arpu Decimal64(2) MATERIALIZED total_revenue / paying_users  -- ê³„ì‚°ëœ ì»¬ëŸ¼
) ENGINE = SummingMergeTree()  -- ìë™ ì§‘ê³„
ORDER BY (date, hour, platform, currency);

-- ğŸ® ì‚¬ìš©ì í–‰ë™ ë¶„ì„ í…Œì´ë¸”
CREATE TABLE user_behavior_summary (
    user_id UInt64,
    date Date,
    session_count UInt16,
    total_playtime_minutes UInt32,
    events_count UInt32,
    revenue_spent Decimal64(2),
    level_start UInt16,
    level_end UInt16,
    last_login DateTime
) ENGINE = ReplacingMergeTree(last_login)  -- ì¤‘ë³µ ì œê±°
ORDER BY (user_id, date);

-- ğŸ† ì‹¤ì‹œê°„ ë­í‚¹ í…Œì´ë¸”
CREATE TABLE player_rankings (
    ranking_type LowCardinality(String),  -- 'level', 'pvp_rating', 'guild_contribution'
    user_id UInt64,
    player_name String,
    score UInt64,
    rank UInt32,
    updated_at DateTime DEFAULT now()
) ENGINE = ReplacingMergeTree(updated_at)
ORDER BY (ranking_type, rank);

-- ğŸ“Š ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œìš© Materialized View
CREATE MATERIALIZED VIEW realtime_dashboard_mv
TO realtime_dashboard_table
AS SELECT
    toStartOfMinute(timestamp) as minute,
    count() as events_count,
    uniq(user_id) as unique_users,
    sum(if(event_type = 4, extractFloat64(JSONExtract(properties, 'price')), 0)) as revenue,
    platform
FROM game_events
GROUP BY minute, platform;

-- ğŸ¯ ì‚¬ìš©ì ë¦¬í…ì…˜ ë¶„ì„ ì¿¼ë¦¬
CREATE VIEW user_retention_view AS
SELECT
    toDate(first_login) as cohort_date,
    dateDiff('day', first_login, activity_date) as day_number,
    count(DISTINCT user_id) as active_users,
    count(DISTINCT user_id) / any(cohort_size) as retention_rate
FROM (
    SELECT 
        user_id,
        min(timestamp) as first_login,
        toDate(timestamp) as activity_date
    FROM game_events 
    WHERE event_type = 0  -- LOGIN
    GROUP BY user_id, activity_date
) a
LEFT JOIN (
    SELECT 
        toDate(min(timestamp)) as cohort_date,
        count(DISTINCT user_id) as cohort_size
    FROM game_events 
    WHERE event_type = 0
    GROUP BY cohort_date
) b ON toDate(first_login) = b.cohort_date
GROUP BY cohort_date, day_number
ORDER BY cohort_date, day_number;
```

### **3.2 ê³ ì„±ëŠ¥ ë¶„ì„ ì¿¼ë¦¬ë“¤**

```sql
-- ğŸ”¥ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ KPI ì¿¼ë¦¬ (1ì´ˆ ì´ë‚´ ì‘ë‹µ)

-- 1ï¸âƒ£ í˜„ì¬ ë™ì ‘ì ìˆ˜ ë° íŠ¸ë Œë“œ
SELECT 
    toStartOfMinute(now() - INTERVAL 10 MINUTE) as time_window,
    count(DISTINCT user_id) as concurrent_users,
    avg(concurrent_users) OVER (ORDER BY time_window ROWS 4 PRECEDING) as trend_avg
FROM game_events 
WHERE timestamp >= now() - INTERVAL 10 MINUTE
  AND event_type IN (0, 1)  -- LOGIN, LOGOUT
GROUP BY time_window
ORDER BY time_window;

-- 2ï¸âƒ£ ì‹¤ì‹œê°„ ë§¤ì¶œ ëª¨ë‹ˆí„°ë§ (ë¶„ë‹¹ ì—…ë°ì´íŠ¸)
SELECT 
    toStartOfHour(timestamp) as hour,
    sum(toFloat64(JSONExtract(properties, 'price'))) as hourly_revenue,
    count() as transaction_count,
    uniq(user_id) as paying_users,
    hourly_revenue / paying_users as arpu
FROM game_events 
WHERE event_type = 4  -- ITEM_PURCHASE
  AND timestamp >= today()
GROUP BY hour
ORDER BY hour DESC
LIMIT 24;

-- 3ï¸âƒ£ ì„œë²„ë³„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
SELECT 
    server_id,
    count() as events_count,
    uniq(user_id) as unique_users,
    avg(toFloat64(JSONExtract(properties, 'response_time_ms'))) as avg_response_time,
    quantile(0.95)(toFloat64(JSONExtract(properties, 'response_time_ms'))) as p95_response_time
FROM game_events 
WHERE timestamp >= now() - INTERVAL 1 HOUR
GROUP BY server_id
HAVING unique_users > 10  -- ìµœì†Œ ì‚¬ìš©ì ìˆ˜ í•„í„°
ORDER BY avg_response_time DESC;

-- 4ï¸âƒ£ ì¹˜íŠ¸ ì˜ì‹¬ í™œë™ íƒì§€
WITH suspicious_activities AS (
    SELECT 
        user_id,
        count() as actions_per_minute,
        uniq(JSONExtract(properties, 'skill_id')) as unique_skills,
        max(toFloat64(JSONExtract(properties, 'damage'))) as max_damage
    FROM game_events 
    WHERE event_type = 3  -- SKILL_USE
      AND timestamp >= now() - INTERVAL 1 MINUTE
    GROUP BY user_id
)
SELECT 
    user_id,
    actions_per_minute,
    unique_skills,
    max_damage,
    'possible_macro' as alert_type
FROM suspicious_activities
WHERE actions_per_minute > 60  -- ë¶„ë‹¹ 60íšŒ ì´ìƒ ì•¡ì…˜
   OR (unique_skills = 1 AND actions_per_minute > 30)  -- ê°™ì€ ìŠ¤í‚¬ ë°˜ë³µ
   OR max_damage > 50000;  -- ë¹„ì •ìƒì  ê³ ë°ë¯¸ì§€

-- 5ï¸âƒ£ A/B í…ŒìŠ¤íŠ¸ ì‹¤ì‹œê°„ ê²°ê³¼
SELECT 
    JSONExtract(properties, 'ab_test_group') as test_group,
    count(DISTINCT user_id) as participants,
    count(DISTINCT if(event_type = 4, user_id, NULL)) as converters,
    converters / participants as conversion_rate,
    sum(if(event_type = 4, toFloat64(JSONExtract(properties, 'price')), 0)) as revenue,
    revenue / participants as revenue_per_user
FROM game_events 
WHERE timestamp >= today()
  AND JSONHas(properties, 'ab_test_group')
GROUP BY test_group
ORDER BY conversion_rate DESC;

-- 6ï¸âƒ£ ê²Œì„ ë°¸ëŸ°ìŠ¤ ë¶„ì„ (ë ˆë²¨ë³„ ì§„í–‰ ì†ë„)
SELECT 
    toInt32(JSONExtract(properties, 'from_level')) as from_level,
    toInt32(JSONExtract(properties, 'to_level')) as to_level,
    count() as levelups_count,
    avg(toFloat64(JSONExtract(properties, 'time_spent_minutes'))) as avg_time_to_levelup,
    quantile(0.5)(toFloat64(JSONExtract(properties, 'time_spent_minutes'))) as median_time
FROM game_events 
WHERE event_type = 2  -- LEVEL_UP
  AND timestamp >= today() - INTERVAL 7 DAY
GROUP BY from_level, to_level
HAVING levelups_count >= 100  -- í†µê³„ì  ìœ ì˜ì„±
ORDER BY from_level, to_level;

-- 7ï¸âƒ£ ì‹¤ì‹œê°„ ì´íƒˆ ìœ„í—˜ ì‚¬ìš©ì ì‹ë³„
WITH user_sessions AS (
    SELECT 
        user_id,
        min(timestamp) as session_start,
        max(timestamp) as session_end,
        dateDiff('minute', session_start, session_end) as session_length,
        count() as events_in_session
    FROM game_events 
    WHERE timestamp >= now() - INTERVAL 2 HOUR
    GROUP BY user_id, session_id
),
recent_behavior AS (
    SELECT 
        user_id,
        avg(session_length) as avg_session_length,
        avg(events_in_session) as avg_events_per_session,
        count() as sessions_count
    FROM user_sessions
    GROUP BY user_id
)
SELECT 
    user_id,
    avg_session_length,
    avg_events_per_session,
    sessions_count,
    'at_risk' as status
FROM recent_behavior
WHERE avg_session_length < 5  -- í‰ê·  5ë¶„ ë¯¸ë§Œ ì„¸ì…˜
   OR avg_events_per_session < 10  -- ì„¸ì…˜ë‹¹ ì´ë²¤íŠ¸ 10ê°œ ë¯¸ë§Œ
   OR sessions_count = 1  -- ì¬ì ‘ì† ì—†ìŒ
ORDER BY avg_session_length ASC;
```

---

## ğŸ¤– 4. ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸

### **4.1 ì‚¬ìš©ì ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸**

```python
# ğŸ§  MLflow ê¸°ë°˜ ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ
import mlflow
import mlflow.sklearn
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder
import clickhouse_connect
import logging
from datetime import datetime, timedelta

class ChurnPredictionPipeline:
    def __init__(self, clickhouse_client):
        self.client = clickhouse_client
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        
        # MLflow ì„¤ì •
        mlflow.set_tracking_uri("http://localhost:5000")
        mlflow.set_experiment("churn_prediction")
    
    def extract_features(self, days_back=30):
        """ClickHouseì—ì„œ ì‚¬ìš©ì íŠ¹ì„± ì¶”ì¶œ"""
        
        # ğŸ¯ ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¿¼ë¦¬
        feature_query = f"""
        WITH user_stats AS (
            SELECT 
                user_id,
                -- ê¸°ë³¸ í†µê³„
                count() as total_events,
                uniq(toDate(timestamp)) as active_days,
                dateDiff('day', min(timestamp), max(timestamp)) + 1 as user_lifetime_days,
                
                -- ì„¸ì…˜ ë¶„ì„
                uniq(session_id) as total_sessions,
                avg(session_length_minutes) as avg_session_length,
                max(session_length_minutes) as max_session_length,
                
                -- ê²Œì„í”Œë ˆì´ íŒ¨í„´
                countIf(event_type = 2) as level_ups,  -- LEVEL_UP
                countIf(event_type = 3) as skill_uses,  -- SKILL_USE
                countIf(event_type = 6) as quest_completes,  -- QUEST_COMPLETE
                countIf(event_type = 9) as pvp_battles,  -- PVP_BATTLE
                
                -- ì†Œì…œ í™œë™
                countIf(event_type = 7) as chat_messages,  -- CHAT_MESSAGE
                countIf(event_type = 8) as guild_activities,  -- GUILD_ACTION
                
                -- ê²½ì œ í™œë™
                countIf(event_type = 4) as purchases,  -- ITEM_PURCHASE
                sum(if(event_type = 4, toFloat64(JSONExtract(properties, 'price')), 0)) as total_spent,
                
                -- ìµœê·¼ í™œë™ (ì§€ë‚œ 7ì¼)
                countIf(timestamp >= now() - INTERVAL 7 DAY) as recent_events,
                countIf(timestamp >= now() - INTERVAL 7 DAY AND event_type = 0) as recent_logins,
                
                -- í”Œë«í¼ ë° ê¸°ê¸° ì •ë³´
                uniq(platform) as platforms_used,
                uniq(JSONExtract(properties, 'device_type')) as devices_used,
                
                -- ì‹œê°„ íŒ¨í„´
                uniq(toHour(timestamp)) as unique_hours_played,
                countIf(toHour(timestamp) BETWEEN 22 AND 6) as late_night_events,
                
                -- ë§ˆì§€ë§‰ í™œë™
                max(timestamp) as last_activity,
                dateDiff('day', max(timestamp), now()) as days_since_last_activity
                
            FROM (
                SELECT 
                    *,
                    -- ì„¸ì…˜ ê¸¸ì´ ê³„ì‚°
                    dateDiff('minute', 
                        lag(timestamp) OVER (PARTITION BY user_id, session_id ORDER BY timestamp),
                        timestamp
                    ) as session_length_minutes
                FROM game_events 
                WHERE timestamp >= now() - INTERVAL {days_back} DAY
            )
            GROUP BY user_id
            HAVING total_events >= 10  -- ìµœì†Œ í™œë™ëŸ‰ í•„í„°
        ),
        churn_labels AS (
            SELECT 
                user_id,
                if(days_since_last_activity >= 7, 1, 0) as is_churned  -- 7ì¼ ì´ìƒ ë¯¸ì ‘ì† = ì´íƒˆ
            FROM user_stats
        )
        SELECT 
            s.*,
            c.is_churned,
            -- íŒŒìƒ í”¼ì²˜ë“¤
            total_events / user_lifetime_days as events_per_day,
            total_spent / greatest(purchases, 1) as avg_purchase_amount,
            recent_events / greatest(total_events, 1) as recent_activity_ratio,
            level_ups / greatest(user_lifetime_days, 1) as levelup_velocity,
            chat_messages / greatest(total_events, 1) as social_engagement_ratio
        FROM user_stats s
        JOIN churn_labels c ON s.user_id = c.user_id
        """
        
        return self.client.query_df(feature_query)
    
    def preprocess_features(self, df):
        """íŠ¹ì„± ì „ì²˜ë¦¬ ë° ì—”ì§€ë‹ˆì–´ë§"""
        
        # ğŸ§¹ ë°ì´í„° ì •ì œ
        df = df.fillna(0)
        
        # ğŸ“Š ì¶”ê°€ íŒŒìƒ ë³€ìˆ˜ ìƒì„±
        df['spending_category'] = pd.cut(df['total_spent'], 
                                       bins=[0, 10, 100, 1000, float('inf')],
                                       labels=['free', 'light_spender', 'moderate_spender', 'whale'])
        
        df['engagement_score'] = (
            df['active_days'] * 0.3 +
            df['avg_session_length'] * 0.2 +
            df['social_engagement_ratio'] * 100 * 0.3 +
            np.log1p(df['total_spent']) * 0.2
        )
        
        df['gameplay_diversity'] = (
            (df['level_ups'] > 0).astype(int) +
            (df['skill_uses'] > 0).astype(int) +
            (df['quest_completes'] > 0).astype(int) +
            (df['pvp_battles'] > 0).astype(int) +
            (df['purchases'] > 0).astype(int)
        )
        
        # ğŸ·ï¸ ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        le = LabelEncoder()
        if 'spending_category' in df.columns:
            df['spending_category_encoded'] = le.fit_transform(df['spending_category'].astype(str))
        
        # ğŸ“ˆ ì •ê·œí™”ê°€ í•„ìš”í•œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤
        numeric_features = [
            'total_events', 'avg_session_length', 'total_spent', 'engagement_score',
            'events_per_day', 'levelup_velocity', 'social_engagement_ratio'
        ]
        
        scaler = StandardScaler()
        df[numeric_features] = scaler.fit_transform(df[numeric_features])
        self.scalers['features'] = scaler
        
        return df
    
    def train_models(self, df):
        """ë‹¤ì¤‘ ëª¨ë¸ í›ˆë ¨ ë° ë¹„êµ"""
        
        # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_columns = [col for col in df.columns 
                          if col not in ['user_id', 'is_churned', 'spending_category']]
        X = df[feature_columns]
        y = df['is_churned']
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ğŸ“Š ëª¨ë¸ë“¤ ì •ì˜
        models_to_train = {
            'random_forest': RandomForestClassifier(
                n_estimators=100, max_depth=10, random_state=42
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=100, max_depth=6, random_state=42
            ),
            'logistic_regression': LogisticRegression(
                random_state=42, max_iter=1000
            )
        }
        
        best_model = None
        best_score = 0
        
        for model_name, model in models_to_train.items():
            with mlflow.start_run(run_name=f"churn_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M')}"):
                
                # ëª¨ë¸ í›ˆë ¨
                model.fit(X_train, y_train)
                
                # ì˜ˆì¸¡ ë° í‰ê°€
                y_pred = model.predict(X_test)
                y_pred_proba = model.predict_proba(X_test)[:, 1]
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                auc_score = roc_auc_score(y_test, y_pred_proba)
                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')
                
                # MLflow ë¡œê¹…
                mlflow.log_param("model_type", model_name)
                mlflow.log_param("train_size", len(X_train))
                mlflow.log_param("test_size", len(X_test))
                mlflow.log_metric("auc_score", auc_score)
                mlflow.log_metric("cv_mean", cv_scores.mean())
                mlflow.log_metric("cv_std", cv_scores.std())
                
                # íŠ¹ì„± ì¤‘ìš”ë„ (Random Forest, Gradient Boostingë§Œ)
                if hasattr(model, 'feature_importances_'):
                    feature_importance = dict(zip(feature_columns, model.feature_importances_))
                    self.feature_importance[model_name] = feature_importance
                    
                    # ìƒìœ„ 10ê°œ íŠ¹ì„± ë¡œê¹…
                    top_features = sorted(feature_importance.items(), 
                                        key=lambda x: x[1], reverse=True)[:10]
                    for i, (feature, importance) in enumerate(top_features):
                        mlflow.log_metric(f"feature_importance_{i+1}", importance)
                        mlflow.log_param(f"top_feature_{i+1}", feature)
                
                # ëª¨ë¸ ì €ì¥
                mlflow.sklearn.log_model(model, f"churn_model_{model_name}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¶”ì 
                if auc_score > best_score:
                    best_score = auc_score
                    best_model = model_name
                    self.models['best_churn_model'] = model
                
                print(f"{model_name} - AUC: {auc_score:.4f}, CV: {cv_scores.mean():.4f}Â±{cv_scores.std():.4f}")
        
        print(f"\nğŸ† Best Model: {best_model} with AUC: {best_score:.4f}")
        return best_model, best_score
    
    def predict_churn_risk(self, user_ids):
        """ì‹¤ì‹œê°„ ì´íƒˆ ìœ„í—˜ë„ ì˜ˆì¸¡"""
        
        if 'best_churn_model' not in self.models:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì‚¬ìš©ìë³„ ìµœì‹  íŠ¹ì„± ì¶”ì¶œ
        user_features_query = f"""
        SELECT 
            user_id,
            count() as total_events,
            dateDiff('day', min(timestamp), max(timestamp)) + 1 as user_lifetime_days,
            uniq(session_id) as total_sessions,
            avg(session_length_minutes) as avg_session_length,
            countIf(event_type = 4) as purchases,
            sum(if(event_type = 4, toFloat64(JSONExtract(properties, 'price')), 0)) as total_spent,
            countIf(timestamp >= now() - INTERVAL 7 DAY) as recent_events,
            dateDiff('day', max(timestamp), now()) as days_since_last_activity
        FROM game_events 
        WHERE user_id IN ({','.join(map(str, user_ids))})
        GROUP BY user_id
        """
        
        df = self.client.query_df(user_features_query)
        df = self.preprocess_features(df)
        
        # ì˜ˆì¸¡
        model = self.models['best_churn_model']
        feature_columns = [col for col in df.columns 
                          if col not in ['user_id', 'is_churned', 'spending_category']]
        
        predictions = model.predict_proba(df[feature_columns])[:, 1]
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        for i, user_id in enumerate(df['user_id']):
            risk_score = predictions[i]
            risk_level = 'high' if risk_score > 0.7 else 'medium' if risk_score > 0.3 else 'low'
            
            results.append({
                'user_id': user_id,
                'churn_probability': risk_score,
                'risk_level': risk_level,
                'predicted_at': datetime.now()
            })
        
        return results
    
    def schedule_daily_predictions(self):
        """ì¼ì¼ ë°°ì¹˜ ì˜ˆì¸¡ ì‘ì—…"""
        
        # í™œì„± ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ
        active_users_query = """
        SELECT DISTINCT user_id 
        FROM game_events 
        WHERE timestamp >= now() - INTERVAL 30 DAY
        LIMIT 10000  -- ë°°ì¹˜ í¬ê¸° ì œí•œ
        """
        
        user_ids = self.client.query_df(active_users_query)['user_id'].tolist()
        
        if not user_ids:
            print("ì˜ˆì¸¡í•  í™œì„± ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤í–‰
        predictions = self.predict_churn_risk(user_ids)
        
        # ê²°ê³¼ë¥¼ ClickHouseì— ì €ì¥
        self.save_predictions_to_db(predictions)
        
        # ê³ ìœ„í—˜ ì‚¬ìš©ì ì•Œë¦¼
        high_risk_users = [p for p in predictions if p['risk_level'] == 'high']
        if high_risk_users:
            self.send_churn_alert(high_risk_users)
    
    def save_predictions_to_db(self, predictions):
        """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        
        # ClickHouse í…Œì´ë¸” ìƒì„± (ìµœì´ˆ 1íšŒ)
        create_table_query = """
        CREATE TABLE IF NOT EXISTS churn_predictions (
            user_id UInt64,
            churn_probability Float64,
            risk_level LowCardinality(String),
            predicted_at DateTime,
            model_version String
        ) ENGINE = ReplacingMergeTree(predicted_at)
        ORDER BY (user_id, predicted_at)
        """
        
        self.client.command(create_table_query)
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì‚½ì…
        for pred in predictions:
            insert_query = f"""
            INSERT INTO churn_predictions VALUES 
            ({pred['user_id']}, {pred['churn_probability']}, 
             '{pred['risk_level']}', '{pred['predicted_at']}', 'v1.0')
            """
            self.client.command(insert_query)
    
    def send_churn_alert(self, high_risk_users):
        """ì´íƒˆ ìœ„í—˜ ì‚¬ìš©ì ì•Œë¦¼"""
        print(f"ğŸš¨ {len(high_risk_users)}ëª…ì˜ ê³ ìœ„í—˜ ì´íƒˆ ì‚¬ìš©ì ê°ì§€!")
        for user in high_risk_users[:5]:  # ìƒìœ„ 5ëª…ë§Œ ì¶œë ¥
            print(f"User {user['user_id']}: {user['churn_probability']:.2%} ì´íƒˆ í™•ë¥ ")

# ğŸš€ ì‚¬ìš© ì˜ˆì œ
def run_churn_prediction_pipeline():
    # ClickHouse ì—°ê²°
    client = clickhouse_connect.get_client(
        host='localhost',
        port=8123,
        database='analytics',
        username='default'
    )
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = ChurnPredictionPipeline(client)
    
    # ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬
    print("ğŸ“Š ì‚¬ìš©ì íŠ¹ì„± ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    df = pipeline.extract_features(days_back=30)
    df = pipeline.preprocess_features(df)
    
    print(f"ì´ {len(df)}ëª…ì˜ ì‚¬ìš©ì ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
    print(f"ì´íƒˆ ì‚¬ìš©ì ë¹„ìœ¨: {df['is_churned'].mean():.2%}")
    
    # ëª¨ë¸ í›ˆë ¨
    print("\nğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
    best_model, best_score = pipeline.train_models(df)
    
    # ì¼ì¼ ì˜ˆì¸¡ ì‹¤í–‰
    print("\nğŸ”® ì¼ì¼ ì´íƒˆ ìœ„í—˜ ì˜ˆì¸¡ ì‹¤í–‰...")
    pipeline.schedule_daily_predictions()

if __name__ == "__main__":
    run_churn_prediction_pipeline()
```

### **4.2 ê²Œì„ ë°¸ëŸ°ìŠ¤ ìµœì í™” ëª¨ë¸**

```python
# âš–ï¸ ê²Œì„ ë°¸ëŸ°ìŠ¤ ìë™ ì¡°ì • ì‹œìŠ¤í…œ
import numpy as np
import pandas as pd
from scipy import optimize
from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns

class GameBalanceOptimizer:
    def __init__(self, clickhouse_client):
        self.client = clickhouse_client
        self.balance_history = []
        self.current_config = {}
    
    def analyze_weapon_balance(self):
        """ë¬´ê¸° ë°¸ëŸ°ìŠ¤ ë¶„ì„"""
        
        weapon_stats_query = """
        WITH weapon_usage AS (
            SELECT 
                JSONExtract(properties, 'weapon_id') as weapon_id,
                JSONExtract(properties, 'damage_dealt') as damage,
                JSONExtract(properties, 'battle_result') as result,
                user_id,
                timestamp
            FROM game_events 
            WHERE event_type = 9  -- PVP_BATTLE
              AND timestamp >= now() - INTERVAL 7 DAY
              AND JSONHas(properties, 'weapon_id')
        )
        SELECT 
            weapon_id,
            count() as usage_count,
            avg(toFloat64(damage)) as avg_damage,
            countIf(result = 'win') / count() as win_rate,
            uniq(user_id) as unique_users,
            
            -- ì¶”ê°€ ë°¸ëŸ°ìŠ¤ ì§€í‘œ
            quantile(0.95)(toFloat64(damage)) as p95_damage,
            stddevPop(toFloat64(damage)) as damage_variance,
            
            -- ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´
            countIf(toHour(timestamp) BETWEEN 20 AND 23) / count() as evening_usage_ratio
        FROM weapon_usage
        GROUP BY weapon_id
        HAVING usage_count >= 100  -- í†µê³„ì  ìœ ì˜ì„± í™•ë³´
        ORDER BY usage_count DESC
        """
        
        weapon_data = self.client.query_df(weapon_stats_query)
        
        # ğŸ¯ ë°¸ëŸ°ìŠ¤ ë¬¸ì œ ê°ì§€
        balance_issues = []
        
        # 1. ìŠ¹ë¥  ë¶ˆê· í˜• (50% Â± 10% ë²—ì–´ë‚˜ëŠ” ë¬´ê¸°ë“¤)
        unbalanced_weapons = weapon_data[
            (weapon_data['win_rate'] < 0.4) | (weapon_data['win_rate'] > 0.6)
        ]
        
        for _, weapon in unbalanced_weapons.iterrows():
            severity = abs(weapon['win_rate'] - 0.5) * 2  # 0~1 ìŠ¤ì¼€ì¼
            balance_issues.append({
                'type': 'win_rate_imbalance',
                'weapon_id': weapon['weapon_id'],
                'current_value': weapon['win_rate'],
                'target_value': 0.5,
                'severity': severity,
                'recommendation': 'damage_adjustment'
            })
        
        # 2. ì‚¬ìš©ë¥  ë¶ˆê· í˜• (íŠ¹ì • ë¬´ê¸°ë§Œ ê³¼ë„í•˜ê²Œ ì‚¬ìš©)
        total_usage = weapon_data['usage_count'].sum()
        expected_usage_rate = 1.0 / len(weapon_data)  # ê· ë“± ë¶„ë°° ê¸°ì¤€
        
        for _, weapon in weapon_data.iterrows():
            actual_usage_rate = weapon['usage_count'] / total_usage
            if actual_usage_rate > expected_usage_rate * 2:  # 2ë°° ì´ìƒ ì‚¬ìš©
                balance_issues.append({
                    'type': 'overused_weapon',
                    'weapon_id': weapon['weapon_id'],
                    'current_value': actual_usage_rate,
                    'target_value': expected_usage_rate,
                    'severity': actual_usage_rate / expected_usage_rate - 1,
                    'recommendation': 'nerf_damage_or_cooldown'
                })
        
        return weapon_data, balance_issues
    
    def optimize_skill_cooldowns(self):
        """ìŠ¤í‚¬ ì¿¨ë‹¤ìš´ ìµœì í™”"""
        
        skill_usage_query = """
        WITH skill_sequences AS (
            SELECT 
                user_id,
                JSONExtract(properties, 'skill_id') as skill_id,
                timestamp,
                lag(timestamp) OVER (PARTITION BY user_id ORDER BY timestamp) as prev_usage,
                dateDiff('second', prev_usage, timestamp) as cooldown_gap
            FROM game_events 
            WHERE event_type = 3  -- SKILL_USE
              AND timestamp >= now() - INTERVAL 3 DAY
        )
        SELECT 
            skill_id,
            count() as total_uses,
            avg(cooldown_gap) as avg_gap_seconds,
            quantile(0.5)(cooldown_gap) as median_gap,
            quantile(0.1)(cooldown_gap) as p10_gap,  -- ë¹ ë¥¸ ì—°ì† ì‚¬ìš©
            
            -- ì „íˆ¬ ìƒí™©ë³„ ë¶„ì„
            countIf(cooldown_gap <= 1) as instant_reuses,
            countIf(cooldown_gap BETWEEN 2 AND 5) as quick_reuses,
            
            -- ì‚¬ìš©ì í–‰ë™ íŒ¨í„´
            uniq(user_id) as unique_users,
            total_uses / unique_users as uses_per_user
        FROM skill_sequences
        WHERE cooldown_gap IS NOT NULL
        GROUP BY skill_id
        HAVING total_uses >= 1000
        ORDER BY instant_reuses DESC
        """
        
        skill_data = self.client.query_df(skill_usage_query)
        
        # ğŸ® ì¿¨ë‹¤ìš´ ìµœì í™” ì•Œê³ ë¦¬ì¦˜
        optimized_cooldowns = {}
        
        for _, skill in skill_data.iterrows():
            skill_id = skill['skill_id']
            current_usage_pattern = {
                'instant_reuses': skill['instant_reuses'],
                'avg_gap': skill['avg_gap_seconds'],
                'uses_per_user': skill['uses_per_user']
            }
            
            # ëª©í‘œ: ì¦‰ì‹œ ì¬ì‚¬ìš© < 5%, í‰ê·  ê°„ê²© > 10ì´ˆ
            if skill['instant_reuses'] > skill['total_uses'] * 0.05:
                # ë„ˆë¬´ ìì£¼ ì‚¬ìš©ë¨ - ì¿¨ë‹¤ìš´ ì¦ê°€ í•„ìš”
                current_cooldown = skill['p10_gap']  # í˜„ì¬ ì‹¤ì œ ìµœì†Œ ì¿¨ë‹¤ìš´ ì¶”ì •
                recommended_cooldown = current_cooldown * 1.5
                
                optimized_cooldowns[skill_id] = {
                    'current_estimated': current_cooldown,
                    'recommended': recommended_cooldown,
                    'reason': 'reduce_spam_usage',
                    'impact_estimate': 'reduce_usage_by_30%'
                }
            
            elif skill['avg_gap'] > 30 and skill['uses_per_user'] < 5:
                # ë„ˆë¬´ ì ê²Œ ì‚¬ìš©ë¨ - ì¿¨ë‹¤ìš´ ê°ì†Œ ê³ ë ¤
                current_cooldown = skill['median_gap']
                recommended_cooldown = max(5, current_cooldown * 0.8)
                
                optimized_cooldowns[skill_id] = {
                    'current_estimated': current_cooldown,
                    'recommended': recommended_cooldown,
                    'reason': 'increase_viability',
                    'impact_estimate': 'increase_usage_by_20%'
                }
        
        return skill_data, optimized_cooldowns
    
    def detect_economy_inflation(self):
        """ê²Œì„ ë‚´ ê²½ì œ ì¸í”Œë ˆì´ì…˜ ê°ì§€"""
        
        economy_query = """
        WITH daily_economy AS (
            SELECT 
                toDate(timestamp) as date,
                sum(if(event_type = 4, toFloat64(JSONExtract(properties, 'price')), 0)) as daily_spent,
                count(DISTINCT if(event_type = 4, user_id, NULL)) as paying_users,
                
                -- ê³¨ë“œ ìƒì„± vs ì†Œëª¨
                sum(if(JSONExtract(properties, 'gold_change') > 0, 
                       toFloat64(JSONExtract(properties, 'gold_change')), 0)) as gold_generated,
                sum(if(JSONExtract(properties, 'gold_change') < 0, 
                       abs(toFloat64(JSONExtract(properties, 'gold_change'))), 0)) as gold_spent,
                
                -- ì•„ì´í…œ ê°€ê²© íŠ¸ë Œë“œ
                avg(if(JSONHas(properties, 'item_market_price'), 
                       toFloat64(JSONExtract(properties, 'item_market_price')), NULL)) as avg_market_price
            FROM game_events 
            WHERE timestamp >= now() - INTERVAL 30 DAY
            GROUP BY date
            ORDER BY date
        )
        SELECT 
            *,
            gold_generated / gold_spent as gold_generation_ratio,
            daily_spent / paying_users as arpu_daily,
            
            -- íŠ¸ë Œë“œ ê³„ì‚° (7ì¼ ì´ë™í‰ê· )
            avg(avg_market_price) OVER (ORDER BY date ROWS 6 PRECEDING) as price_trend_7d,
            avg(gold_generation_ratio) OVER (ORDER BY date ROWS 6 PRECEDING) as gold_ratio_trend_7d
        FROM daily_economy
        """
        
        economy_data = self.client.query_df(economy_query)
        
        # ğŸ“ˆ ì¸í”Œë ˆì´ì…˜ ì§€í‘œ ê³„ì‚°
        inflation_indicators = {}
        
        if len(economy_data) >= 14:  # ìµœì†Œ 2ì£¼ ë°ì´í„°
            # 1. ì‹œì¥ ê°€ê²© ìƒìŠ¹ë¥ 
            recent_prices = economy_data.tail(7)['avg_market_price'].mean()
            older_prices = economy_data.head(7)['avg_market_price'].mean()
            price_inflation = (recent_prices - older_prices) / older_prices if older_prices > 0 else 0
            
            # 2. ê³¨ë“œ ìƒì„±/ì†Œëª¨ ë¹„ìœ¨ ë³€í™”
            recent_gold_ratio = economy_data.tail(7)['gold_generation_ratio'].mean()
            older_gold_ratio = economy_data.head(7)['gold_generation_ratio'].mean()
            
            inflation_indicators = {
                'price_inflation_rate': price_inflation,
                'current_gold_ratio': recent_gold_ratio,
                'gold_ratio_change': recent_gold_ratio - older_gold_ratio,
                'risk_level': self._assess_inflation_risk(price_inflation, recent_gold_ratio),
                'recommendations': self._generate_economy_recommendations(price_inflation, recent_gold_ratio)
            }
        
        return economy_data, inflation_indicators
    
    def _assess_inflation_risk(self, price_inflation, gold_ratio):
        """ì¸í”Œë ˆì´ì…˜ ìœ„í—˜ë„ í‰ê°€"""
        if price_inflation > 0.1 or gold_ratio > 1.5:  # 10% ê°€ê²© ìƒìŠ¹ ë˜ëŠ” ê³¨ë“œ ê³¼ë‹¤ ìƒì„±
            return 'high'
        elif price_inflation > 0.05 or gold_ratio > 1.2:
            return 'medium'
        else:
            return 'low'
    
    def _generate_economy_recommendations(self, price_inflation, gold_ratio):
        """ê²½ì œ ì •ì±… ì¶”ì²œ"""
        recommendations = []
        
        if price_inflation > 0.1:
            recommendations.append({
                'action': 'reduce_gold_rewards',
                'target': 'quest_rewards',
                'adjustment': '-20%',
                'reason': 'combat_price_inflation'
            })
        
        if gold_ratio > 1.5:
            recommendations.append({
                'action': 'add_gold_sinks',
                'target': 'repair_costs_and_taxes',
                'adjustment': '+30%',
                'reason': 'reduce_gold_surplus'
            })
        
        if not recommendations:
            recommendations.append({
                'action': 'monitor_only',
                'reason': 'economy_stable'
            })
        
        return recommendations
    
    def generate_balance_report(self):
        """ì¢…í•© ë°¸ëŸ°ìŠ¤ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        print("ğŸ® ê²Œì„ ë°¸ëŸ°ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸")
        print("=" * 50)
        
        # ë¬´ê¸° ë°¸ëŸ°ìŠ¤ ë¶„ì„
        weapon_data, weapon_issues = self.analyze_weapon_balance()
        print(f"\nâš”ï¸  ë¬´ê¸° ë¶„ì„: {len(weapon_data)}ê°œ ë¬´ê¸°, {len(weapon_issues)}ê°œ ì´ìŠˆ ë°œê²¬")
        
        for issue in weapon_issues[:3]:  # ìƒìœ„ 3ê°œ ì´ìŠˆë§Œ ì¶œë ¥
            print(f"  - {issue['weapon_id']}: {issue['type']} (ì‹¬ê°ë„: {issue['severity']:.2f})")
        
        # ìŠ¤í‚¬ ì¿¨ë‹¤ìš´ ìµœì í™”
        skill_data, cooldown_opts = self.optimize_skill_cooldowns()
        print(f"\nğŸ¯ ìŠ¤í‚¬ ë¶„ì„: {len(skill_data)}ê°œ ìŠ¤í‚¬, {len(cooldown_opts)}ê°œ ì¡°ì • ì œì•ˆ")
        
        for skill_id, opt in list(cooldown_opts.items())[:3]:
            print(f"  - {skill_id}: {opt['current_estimated']:.1f}ì´ˆ â†’ {opt['recommended']:.1f}ì´ˆ ({opt['reason']})")
        
        # ê²½ì œ ì‹œìŠ¤í…œ ë¶„ì„
        economy_data, inflation_info = self.detect_economy_inflation()
        if inflation_info:
            print(f"\nğŸ’° ê²½ì œ ë¶„ì„: ì¸í”Œë ˆì´ì…˜ ìœ„í—˜ë„ {inflation_info['risk_level']}")
            print(f"  - ê°€ê²© ìƒìŠ¹ë¥ : {inflation_info['price_inflation_rate']:.1%}")
            print(f"  - ê³¨ë“œ ìƒì„±ë¹„: {inflation_info['current_gold_ratio']:.2f}")
        
        # ì¢…í•© ì¶”ì²œì‚¬í•­
        print(f"\nğŸ“‹ ì¶”ì²œì‚¬í•­:")
        print(f"  1. ë¬´ê¸° ë°¸ëŸ°ìŠ¤: {len([i for i in weapon_issues if i['severity'] > 0.3])}ê°œ ê¸´ê¸‰ ì¡°ì¹˜ í•„ìš”")
        print(f"  2. ìŠ¤í‚¬ ì¡°ì •: {len(cooldown_opts)}ê°œ ìŠ¤í‚¬ ì¿¨ë‹¤ìš´ ì¬ì¡°ì •")
        print(f"  3. ê²½ì œ ì •ì±…: {len(inflation_info.get('recommendations', []))}ê°œ ì •ì±… ì œì•ˆ")
        
        return {
            'weapon_balance': weapon_data,
            'weapon_issues': weapon_issues,
            'skill_optimization': cooldown_opts,
            'economy_status': inflation_info
        }

# ğŸš€ ì‹¤í–‰ ì˜ˆì œ
def run_balance_analysis():
    client = clickhouse_connect.get_client(
        host='localhost', port=8123, database='analytics'
    )
    
    optimizer = GameBalanceOptimizer(client)
    balance_report = optimizer.generate_balance_report()
    
    return balance_report

if __name__ == "__main__":
    run_balance_analysis()
```

---

## ğŸ“Š 5. ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

### **5.1 React ê¸°ë°˜ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ**

```typescript
// ğŸš€ ì‹¤ì‹œê°„ ê²Œì„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
import React, { useState, useEffect, useRef } from 'react';
import {
  Chart as ChartJS,
  CategoryScale,
  LinearScale,
  PointElement,
  LineElement,
  BarElement,
  Title,
  Tooltip,
  Legend,
  TimeScale,
  ArcElement
} from 'chart.js';
import { Line, Bar, Doughnut } from 'react-chartjs-2';
import 'chartjs-adapter-date-fns';

ChartJS.register(
  CategoryScale, LinearScale, PointElement, LineElement, BarElement,
  Title, Tooltip, Legend, TimeScale, ArcElement
);

// ğŸ“¡ ì‹¤ì‹œê°„ ë°ì´í„° íƒ€ì… ì •ì˜
interface RealTimeMetrics {
  timestamp: string;
  concurrent_users: number;
  events_per_second: number;
  revenue_per_minute: number;
  server_response_time: number;
  error_rate: number;
}

interface ChurnPrediction {
  user_id: number;
  churn_probability: number;
  risk_level: 'low' | 'medium' | 'high';
  predicted_at: string;
}

interface ServerHealth {
  server_id: string;
  cpu_usage: number;
  memory_usage: number;
  active_connections: number;
  status: 'healthy' | 'warning' | 'critical';
}

// ğŸ® ë©”ì¸ ëŒ€ì‹œë³´ë“œ ì»´í¬ë„ŒíŠ¸
const GameAnalyticsDashboard: React.FC = () => {
  const [metrics, setMetrics] = useState<RealTimeMetrics[]>([]);
  const [churnData, setChurnData] = useState<ChurnPrediction[]>([]);
  const [serverHealth, setServerHealth] = useState<ServerHealth[]>([]);
  const [isConnected, setIsConnected] = useState(false);
  const wsRef = useRef<WebSocket | null>(null);

  // ğŸ”Œ WebSocket ì—°ê²° ì„¤ì •
  useEffect(() => {
    connectWebSocket();
    return () => {
      if (wsRef.current) {
        wsRef.current.close();
      }
    };
  }, []);

  const connectWebSocket = () => {
    try {
      wsRef.current = new WebSocket('ws://localhost:8080/analytics');
      
      wsRef.current.onopen = () => {
        setIsConnected(true);
        console.log('ğŸ“¡ ì‹¤ì‹œê°„ ë°ì´í„° ì—°ê²° ì„±ê³µ');
      };

      wsRef.current.onmessage = (event) => {
        const data = JSON.parse(event.data);
        handleRealTimeData(data);
      };

      wsRef.current.onclose = () => {
        setIsConnected(false);
        console.log('ğŸ”Œ ì—°ê²° ëŠê¹€, 5ì´ˆ í›„ ì¬ì—°ê²° ì‹œë„');
        setTimeout(connectWebSocket, 5000);
      };

      wsRef.current.onerror = (error) => {
        console.error('WebSocket ì˜¤ë¥˜:', error);
      };
    } catch (error) {
      console.error('WebSocket ì—°ê²° ì‹¤íŒ¨:', error);
    }
  };

  // ğŸ“Š ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
  const handleRealTimeData = (data: any) => {
    switch (data.type) {
      case 'metrics':
        setMetrics(prev => {
          const newMetrics = [...prev, data.payload];
          // ìµœê·¼ 100ê°œ ë°ì´í„°í¬ì¸íŠ¸ë§Œ ìœ ì§€
          return newMetrics.slice(-100);
        });
        break;

      case 'churn_predictions':
        setChurnData(data.payload);
        break;

      case 'server_health':
        setServerHealth(data.payload);
        break;

      default:
        console.log('ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° íƒ€ì…:', data.type);
    }
  };

  // ğŸ“ˆ ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„
  const prepareConcurrentUsersChart = () => {
    const last24Hours = metrics.slice(-144); // 10ë¶„ ê°„ê²©ìœ¼ë¡œ 24ì‹œê°„

    return {
      labels: last24Hours.map(m => new Date(m.timestamp)),
      datasets: [
        {
          label: 'ë™ì ‘ì ìˆ˜',
          data: last24Hours.map(m => m.concurrent_users),
          borderColor: 'rgb(75, 192, 192)',
          backgroundColor: 'rgba(75, 192, 192, 0.2)',
          tension: 0.4,
          fill: true
        }
      ]
    };
  };

  const prepareRevenueChart = () => {
    const hourlyRevenue = metrics.reduce((acc, metric) => {
      const hour = new Date(metric.timestamp).getHours();
      if (!acc[hour]) acc[hour] = 0;
      acc[hour] += metric.revenue_per_minute;
      return acc;
    }, {} as Record<number, number>);

    return {
      labels: Array.from({length: 24}, (_, i) => `${i}:00`),
      datasets: [
        {
          label: 'ì‹œê°„ë‹¹ ë§¤ì¶œ',
          data: Array.from({length: 24}, (_, i) => hourlyRevenue[i] || 0),
          backgroundColor: 'rgba(54, 162, 235, 0.6)',
          borderColor: 'rgba(54, 162, 235, 1)',
          borderWidth: 2
        }
      ]
    };
  };

  const prepareChurnRiskChart = () => {
    const riskCounts = churnData.reduce((acc, user) => {
      acc[user.risk_level] = (acc[user.risk_level] || 0) + 1;
      return acc;
    }, {} as Record<string, number>);

    return {
      labels: ['ì €ìœ„í—˜', 'ì¤‘ìœ„í—˜', 'ê³ ìœ„í—˜'],
      datasets: [
        {
          data: [riskCounts.low || 0, riskCounts.medium || 0, riskCounts.high || 0],
          backgroundColor: ['#4CAF50', '#FF9800', '#F44336'],
          borderWidth: 2
        }
      ]
    };
  };

  // ğŸ¨ ì°¨íŠ¸ ì˜µì…˜
  const chartOptions = {
    responsive: true,
    maintainAspectRatio: false,
    plugins: {
      legend: {
        position: 'top' as const,
      },
      title: {
        display: true,
        text: 'ì‹¤ì‹œê°„ ê²Œì„ ë©”íŠ¸ë¦­'
      }
    },
    scales: {
      x: {
        type: 'time' as const,
        time: {
          displayFormats: {
            minute: 'HH:mm',
            hour: 'HH:mm'
          }
        }
      },
      y: {
        beginAtZero: true
      }
    }
  };

  // ğŸš¨ ì•Œë¦¼ ìƒíƒœ ê³„ì‚°
  const getAlertStatus = () => {
    const latestMetrics = metrics[metrics.length - 1];
    if (!latestMetrics) return 'unknown';

    const alerts = [];
    
    if (latestMetrics.concurrent_users < 1000) alerts.push('low_users');
    if (latestMetrics.server_response_time > 200) alerts.push('high_latency');
    if (latestMetrics.error_rate > 0.05) alerts.push('high_errors');

    const highRiskUsers = churnData.filter(u => u.risk_level === 'high').length;
    if (highRiskUsers > 50) alerts.push('high_churn_risk');

    return alerts.length === 0 ? 'healthy' : alerts.length < 2 ? 'warning' : 'critical';
  };

  return (
    <div className="min-h-screen bg-gray-100 p-6">
      {/* ğŸ“± í—¤ë” */}
      <div className="mb-8">
        <div className="flex items-center justify-between">
          <h1 className="text-3xl font-bold text-gray-900">ğŸ® ê²Œì„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ</h1>
          <div className="flex items-center space-x-4">
            <div className={`flex items-center space-x-2 px-3 py-1 rounded-full text-sm ${
              isConnected ? 'bg-green-100 text-green-800' : 'bg-red-100 text-red-800'
            }`}>
              <div className={`w-2 h-2 rounded-full ${
                isConnected ? 'bg-green-500' : 'bg-red-500'
              }`} />
              <span>{isConnected ? 'ì‹¤ì‹œê°„ ì—°ê²°' : 'ì—°ê²° ëŠê¹€'}</span>
            </div>
            <div className={`px-3 py-1 rounded-full text-sm ${
              getAlertStatus() === 'healthy' ? 'bg-green-100 text-green-800' :
              getAlertStatus() === 'warning' ? 'bg-yellow-100 text-yellow-800' :
              'bg-red-100 text-red-800'
            }`}>
              ì‹œìŠ¤í…œ ìƒíƒœ: {getAlertStatus() === 'healthy' ? 'ì •ìƒ' : 
                           getAlertStatus() === 'warning' ? 'ì£¼ì˜' : 'ìœ„í—˜'}
            </div>
          </div>
        </div>
      </div>

      {/* ğŸ“Š KPI ì¹´ë“œë“¤ */}
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6 mb-8">
        <KPICard
          title="í˜„ì¬ ë™ì ‘ì"
          value={metrics[metrics.length - 1]?.concurrent_users || 0}
          change="+12.3%"
          trend="up"
          icon="ğŸ‘¥"
        />
        <KPICard
          title="ë¶„ë‹¹ ì´ë²¤íŠ¸"
          value={metrics[metrics.length - 1]?.events_per_second * 60 || 0}
          change="+5.7%"
          trend="up"
          icon="âš¡"
        />
        <KPICard
          title="ë¶„ë‹¹ ë§¤ì¶œ"
          value={`$${(metrics[metrics.length - 1]?.revenue_per_minute || 0).toFixed(2)}`}
          change="-2.1%"
          trend="down"
          icon="ğŸ’°"
        />
        <KPICard
          title="ê³ ìœ„í—˜ ì‚¬ìš©ì"
          value={churnData.filter(u => u.risk_level === 'high').length}
          change="+8.4%"
          trend="up"
          icon="ğŸš¨"
        />
      </div>

      {/* ğŸ“ˆ ë©”ì¸ ì°¨íŠ¸ë“¤ */}
      <div className="grid grid-cols-1 lg:grid-cols-2 gap-6 mb-8">
        <ChartCard title="ì‹¤ì‹œê°„ ë™ì ‘ì ì¶”ì´">
          <Line data={prepareConcurrentUsersChart()} options={chartOptions} />
        </ChartCard>

        <ChartCard title="ì‹œê°„ëŒ€ë³„ ë§¤ì¶œ">
          <Bar data={prepareRevenueChart()} options={{
            ...chartOptions,
            scales: { y: { beginAtZero: true } }
          }} />
        </ChartCard>

        <ChartCard title="ì´íƒˆ ìœ„í—˜ë„ ë¶„í¬">
          <Doughnut data={prepareChurnRiskChart()} options={{
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { position: 'bottom' }
            }
          }} />
        </ChartCard>

        <ChartCard title="ì„œë²„ ìƒíƒœ">
          <ServerHealthDisplay servers={serverHealth} />
        </ChartCard>
      </div>

      {/* ğŸ“‹ ìƒì„¸ ë°ì´í„° í…Œì´ë¸” */}
      <div className="bg-white rounded-lg shadow p-6">
        <h3 className="text-lg font-semibold mb-4">ğŸ¯ ê³ ìœ„í—˜ ì´íƒˆ ì‚¬ìš©ì ëª©ë¡</h3>
        <ChurnRiskTable data={churnData.filter(u => u.risk_level === 'high').slice(0, 10)} />
      </div>
    </div>
  );
};

// ğŸ¯ KPI ì¹´ë“œ ì»´í¬ë„ŒíŠ¸
interface KPICardProps {
  title: string;
  value: number | string;
  change: string;
  trend: 'up' | 'down';
  icon: string;
}

const KPICard: React.FC<KPICardProps> = ({ title, value, change, trend, icon }) => (
  <div className="bg-white rounded-lg shadow p-6">
    <div className="flex items-center justify-between">
      <div>
        <p className="text-sm text-gray-600">{title}</p>
        <p className="text-2xl font-bold text-gray-900">{value}</p>
        <p className={`text-sm ${trend === 'up' ? 'text-green-600' : 'text-red-600'}`}>
          {change} from last hour
        </p>
      </div>
      <div className="text-3xl">{icon}</div>
    </div>
  </div>
);

// ğŸ“Š ì°¨íŠ¸ ì¹´ë“œ ì»´í¬ë„ŒíŠ¸
interface ChartCardProps {
  title: string;
  children: React.ReactNode;
}

const ChartCard: React.FC<ChartCardProps> = ({ title, children }) => (
  <div className="bg-white rounded-lg shadow p-6">
    <h3 className="text-lg font-semibold mb-4">{title}</h3>
    <div className="h-64">{children}</div>
  </div>
);

// ğŸ–¥ï¸ ì„œë²„ ìƒíƒœ í‘œì‹œ ì»´í¬ë„ŒíŠ¸
const ServerHealthDisplay: React.FC<{ servers: ServerHealth[] }> = ({ servers }) => (
  <div className="space-y-4">
    {servers.map(server => (
      <div key={server.server_id} className="flex items-center justify-between p-3 bg-gray-50 rounded">
        <div>
          <p className="font-medium">{server.server_id}</p>
          <p className="text-sm text-gray-600">
            CPU: {server.cpu_usage}% | Memory: {server.memory_usage}%
          </p>
        </div>
        <div className={`px-2 py-1 rounded text-xs ${
          server.status === 'healthy' ? 'bg-green-100 text-green-800' :
          server.status === 'warning' ? 'bg-yellow-100 text-yellow-800' :
          'bg-red-100 text-red-800'
        }`}>
          {server.status}
        </div>
      </div>
    ))}
  </div>
);

// ğŸ“‹ ì´íƒˆ ìœ„í—˜ í…Œì´ë¸” ì»´í¬ë„ŒíŠ¸
const ChurnRiskTable: React.FC<{ data: ChurnPrediction[] }> = ({ data }) => (
  <div className="overflow-x-auto">
    <table className="min-w-full divide-y divide-gray-200">
      <thead className="bg-gray-50">
        <tr>
          <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase">ì‚¬ìš©ì ID</th>
          <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase">ì´íƒˆ í™•ë¥ </th>
          <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase">ìœ„í—˜ë„</th>
          <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase">ì˜ˆì¸¡ ì‹œê°„</th>
          <th className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase">ì•¡ì…˜</th>
        </tr>
      </thead>
      <tbody className="bg-white divide-y divide-gray-200">
        {data.map(user => (
          <tr key={user.user_id}>
            <td className="px-6 py-4 whitespace-nowrap text-sm font-medium text-gray-900">
              {user.user_id}
            </td>
            <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
              {(user.churn_probability * 100).toFixed(1)}%
            </td>
            <td className="px-6 py-4 whitespace-nowrap">
              <span className={`px-2 py-1 text-xs rounded-full ${
                user.risk_level === 'high' ? 'bg-red-100 text-red-800' :
                user.risk_level === 'medium' ? 'bg-yellow-100 text-yellow-800' :
                'bg-green-100 text-green-800'
              }`}>
                {user.risk_level}
              </span>
            </td>
            <td className="px-6 py-4 whitespace-nowrap text-sm text-gray-500">
              {new Date(user.predicted_at).toLocaleString()}
            </td>
            <td className="px-6 py-4 whitespace-nowrap text-sm text-blue-600">
              <button className="hover:text-blue-800">ë¦¬í…ì…˜ ìº í˜ì¸ ë°œì†¡</button>
            </td>
          </tr>
        ))}
      </tbody>
    </table>
  </div>
);

export default GameAnalyticsDashboard;
```

---

## ğŸš€ 6. ì‹¤ì „ í”„ë¡œì íŠ¸: ì¢…í•© ë¶„ì„ ì‹œìŠ¤í…œ

### **6.1 í”„ë¡œì íŠ¸ ëª©í‘œ**
- **ì‹¤ì‹œê°„ ê²Œì„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘**: ì´ˆë‹¹ 10,000+ ì´ë²¤íŠ¸ ì²˜ë¦¬
- **ì‚¬ìš©ì ì´íƒˆ ì˜ˆì¸¡**: 85% ì´ìƒ ì •í™•ë„
- **ê²Œì„ ë°¸ëŸ°ìŠ¤ ìµœì í™”**: ìë™ ì¶”ì²œ ì‹œìŠ¤í…œ
- **ë§¤ì¶œ ìµœì í™”**: A/B í…ŒìŠ¤íŠ¸ ìë™í™”

### **6.2 ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**
```
Game Clients â†’ Kafka â†’ Spark Streaming â†’ ClickHouse
                â†“             â†“              â†“
            ML Pipeline â†’ Model Serving â†’ Dashboard
```

### **6.3 ê¸°ëŒ€ ì„±ê³¼**
- **ìš´ì˜ íš¨ìœ¨ì„±**: ëª¨ë‹ˆí„°ë§ ì‹œê°„ 80% ë‹¨ì¶•
- **ì‚¬ìš©ì ìœ ì§€ìœ¨**: ì´íƒˆë¥  30% ê°ì†Œ
- **ë§¤ì¶œ ì¦ê°€**: ë°ì´í„° ê¸°ë°˜ ìµœì í™”ë¡œ 20% ë§¤ì¶œ í–¥ìƒ
- **ê²Œì„ í’ˆì§ˆ**: ë°¸ëŸ°ìŠ¤ ì´ìŠˆ ì¡°ê¸° ë°œê²¬ìœ¼ë¡œ í”Œë ˆì´ì–´ ë§Œì¡±ë„ í–¥ìƒ

---

## ğŸ¯ ë§ˆë¬´ë¦¬

**ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!** ì´ì œ ì—¬ëŸ¬ë¶„ì€ **ê²Œì„ ì—…ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì‹¤ì‹œê°„ ë¶„ì„ ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

### **ìŠµë“í•œ í•µì‹¬ ì—­ëŸ‰:**
- âœ… **ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬**: Apache Sparkë¡œ ì´ˆë‹¹ ë§Œê°œ ì´ë²¤íŠ¸ ì²˜ë¦¬
- âœ… **ì‹¤ì‹œê°„ ë¶„ì„**: ClickHouseë¡œ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ì‘ë‹µ
- âœ… **ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸**: ì´íƒˆ ì˜ˆì¸¡, ë°¸ëŸ°ìŠ¤ ìµœì í™” ìë™í™”
- âœ… **ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ**: Reactë¡œ ì„ì›ì§„ìš© ì¸ì‚¬ì´íŠ¸ ì œê³µ
- âœ… **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**: ë°ì´í„°ë¡œ ë§¤ì¶œê³¼ ì‚¬ìš©ì ë§Œì¡±ë„ ë™ì‹œ í–¥ìƒ

### **ì‹¤ì œ ì‚°ì—… í™œìš©ë„:**
- **ê²Œì„ íšŒì‚¬**: ë„¥ìŠ¨, ì—”ì”¨ì†Œí”„íŠ¸, í¬ë˜í”„í†¤ì˜ ë°ì´í„° íŒ€ ìˆ˜ì¤€
- **í…Œí¬ ê¸°ì—…**: Netflix, Uberì˜ ì‹¤ì‹œê°„ ë¶„ì„ ì‹œìŠ¤í…œê³¼ ë™ê¸‰
- **ìŠ¤íƒ€íŠ¸ì—…**: ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ìœ¼ë¡œ ë¹ ë¥¸ ì„±ì¥ ì‹¤í˜„

### **ë‹¤ìŒ ì„±ì¥ ë°©í–¥:**
1. **A/B í…ŒìŠ¤íŠ¸ ìë™í™”** ì‹œìŠ¤í…œ êµ¬ì¶•
2. **ê°œì¸í™” ì¶”ì²œ ì—”ì§„** ê°œë°œ
3. **ì‚¬ê¸° íƒì§€ ì‹œìŠ¤í…œ** ê³ ë„í™”
4. **ì˜ˆì¸¡ ë¶„ì„** ëª¨ë¸ ë‹¤ì–‘í™”

**ğŸ’¼ ì´ ìˆ˜ì¤€ì˜ ì—­ëŸ‰ì´ë©´ ê²Œì„ ì—…ê³„ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸/ë¶„ì„ê°€ë¡œ ì¶©ë¶„íˆ ê²½ìŸë ¥ì´ ìˆìŠµë‹ˆë‹¤!**

---

## ğŸ”¥ í”í•œ ì‹¤ìˆ˜ì™€ í•´ê²°ë°©ë²• (Common Mistakes & Solutions)

### 1. ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ ì‹¤íŒ¨
```cpp
// [SEQUENCE: 1] âŒ ì˜ëª»ëœ ì˜ˆ: ìœ ì—°í•˜ì§€ ëª»í•œ ì´ë²¤íŠ¸ êµ¬ì¡°
struct BadEventSchema {
    int user_id;
    int event_type;  // í•˜ë“œì½”ë”©ëœ íƒ€ì…
    float value1;
    float value2;
    // ìƒˆ í•„ë“œ ì¶”ê°€ ë¶ˆê°€ëŠ¥!
};

// âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: í™•ì¥ ê°€ëŠ¥í•œ JSON ìŠ¤í‚¤ë§ˆ
struct GoodEventSchema {
    std::string event_id;
    std::string event_type;
    std::string user_id;
    uint21_t timestamp;
    nlohmann::json properties;  // ìœ ì—°í•œ ì†ì„±
    
    // ìŠ¤í‚¤ë§ˆ ë²„ì „ ê´€ë¦¬
    std::string schema_version = "2.0";
    
    // ê²€ì¦ í•¨ìˆ˜
    bool Validate() const {
        return !event_id.empty() && 
               !event_type.empty() && 
               timestamp > 0;
    }
};
```

### 2. ë°ì´í„° ì²˜ë¦¬ ë³‘ëª©
```cpp
// [SEQUENCE: 2] âŒ ì˜ëª»ëœ ì˜ˆ: ë™ê¸°ì  ì²˜ë¦¬ë¡œ ë³‘ëª© ë°œìƒ
class BadDataProcessor {
    void ProcessEvent(const GameEvent& event) {
        // ë§¤ ì´ë²¤íŠ¸ë§ˆë‹¤ DB ì“°ê¸° - ì´ˆë‹¹ 100ê°œë§Œ ì²˜ë¦¬ ê°€ëŠ¥
        database.Insert(event);
        
        // ë™ê¸°ì  ì§‘ê³„ - ëŠë¦¼!
        UpdateStatisticsSync(event);
        
        // ë¸”ë¡œí‚¹ ML ì¶”ë¡ 
        auto prediction = ml_model.Predict(event);
    }
};

// âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬
class GoodDataProcessor {
    ThreadSafeQueue<GameEvent> event_buffer;
    
    void ProcessEvent(const GameEvent& event) {
        // ë²„í¼ì— ì¶”ê°€ (ë…¼ë¸”ë¡œí‚¹)
        event_buffer.Push(event);
    }
    
    void BatchProcessor() {
        while (running) {
            auto batch = event_buffer.PopBatch(1000, 100ms);
            
            // ë°°ì¹˜ DB ì‚½ì…
            database.BulkInsert(batch);
            
            // ë³‘ë ¬ ì§‘ê³„
            std::async(std::launch::async, [batch] {
                UpdateStatisticsAsync(batch);
            });
            
            // ë¹„ë™ê¸° ML ì¶”ë¡ 
            ml_pipeline.ProcessBatch(batch);
        }
    }
};
```

### 3. ì˜ëª»ëœ ë°ì´í„° ë³´ì¡´ ì •ì±…
```cpp
// [SEQUENCE: 3] âŒ ì˜ëª»ëœ ì˜ˆ: ëª¨ë“  ë°ì´í„° ë¬´ì œí•œ ë³´ê´€
class BadDataRetention {
    void StoreEvent(const GameEvent& event) {
        // ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ì˜êµ¬ ë³´ê´€ - ìŠ¤í† ë¦¬ì§€ í­ë°œ!
        raw_storage.Store(event);
        
        // ì§‘ê³„ ë°ì´í„°ë„ ì „ë¶€ ë³´ê´€
        aggregated_storage.Store(AggregateAll(event));
    }
};

// âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: ê³„ì¸µì  ë°ì´í„° ë³´ì¡´
class GoodDataRetention {
    void ApplyRetentionPolicy() {
        // Hot: 7ì¼ (ì‹¤ì‹œê°„ ë¶„ì„)
        if (age > 7_days) {
            MoveToWarm(event);
        }
        
        // Warm: 30ì¼ (ì§‘ê³„ëœ ë°ì´í„°)
        if (age > 30_days) {
            auto aggregated = Aggregate(event);
            MoveToCold(aggregated);
            Delete(raw_event);
        }
        
        // Cold: 1ë…„ (ì••ì¶•ëœ ìš”ì•½)
        if (age > 1_year) {
            auto summary = Summarize(event);
            ArchiveToS3(summary);
            Delete(aggregated);
        }
    }
};
```

---

## ğŸš€ ì‹¤ìŠµ í”„ë¡œì íŠ¸ (Practice Projects)

### ğŸ“š ê¸°ì´ˆ í”„ë¡œì íŠ¸: ì‹¤ì‹œê°„ ê²Œì„ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ
**ëª©í‘œ**: ê¸°ë³¸ì ì¸ ê²Œì„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì‹œê°í™”

```cpp
// êµ¬í˜„ ìš”êµ¬ì‚¬í•­:
// 1. ê²Œì„ ì´ë²¤íŠ¸ ìˆ˜ì§‘ API
// 2. Kafkaë¡œ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°
// 3. ì‹¤ì‹œê°„ DAU/MAU ê³„ì‚°
// 4. Grafana ëŒ€ì‹œë³´ë“œ
// 5. ì•Œë¦¼ ì‹œìŠ¤í…œ
// 6. ì´ˆë‹¹ 1,000 ì´ë²¤íŠ¸ ì²˜ë¦¬
```

### ğŸ® ì¤‘ê¸‰ í”„ë¡œì íŠ¸: ìœ ì € ì´íƒˆ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
**ëª©í‘œ**: ML ê¸°ë°˜ ì´íƒˆ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸

```python
# í•µì‹¬ ê¸°ëŠ¥:
# 1. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸
# 2. ì‹¤ì‹œê°„ ëª¨ë¸ í•™ìŠµ
# 3. A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
# 4. ì˜ˆì¸¡ ê²°ê³¼ API
# 5. ë¦¬í…ì…˜ ìº í˜ì¸ ìë™í™”
# 6. 85% ì´ìƒ ì •í™•ë„
```

### ğŸ† ê³ ê¸‰ í”„ë¡œì íŠ¸: ì¢…í•© ê²Œì„ ë¶„ì„ í”Œë«í¼
**ëª©í‘œ**: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë¶„ì„ ì‹œìŠ¤í…œ

```yaml
êµ¬í˜„ ë‚´ìš©:
  - ë©€í‹° ê²Œì„ ì§€ì›
  - ì‹¤ì‹œê°„ ì´ìƒ íƒì§€
  - ìë™ ë°¸ëŸ°ìŠ¤ ë¶„ì„
  - ë§¤ì¶œ ìµœì í™” ì—”ì§„
  - ì»¤ìŠ¤í…€ ì¿¼ë¦¬ ì—”ì§„
  - PBê¸‰ ë°ì´í„° ì²˜ë¦¬
  - ë©€í‹° ë¦¬ì „ ì§€ì›
```

---

## ğŸ“Š í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸ (Learning Checklist)

### ğŸ¥‰ ë¸Œë¡ ì¦ˆ ë ˆë²¨
- [ ] Kafka ê¸°ë³¸ ì‚¬ìš©ë²•
- [ ] ê°„ë‹¨í•œ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
- [ ] ê¸°ë³¸ SQL ì§‘ê³„
- [ ] Grafana ëŒ€ì‹œë³´ë“œ

### ğŸ¥ˆ ì‹¤ë²„ ë ˆë²¨
- [ ] Spark Streaming
- [ ] ClickHouse ì¿¼ë¦¬
- [ ] íŒŒì´ì¬ ë°ì´í„° ë¶„ì„
- [ ] ê¸°ì´ˆ ML ëª¨ë¸

### ğŸ¥‡ ê³¨ë“œ ë ˆë²¨
- [ ] ë³µì¡í•œ ETL íŒŒì´í”„ë¼ì¸
- [ ] ì‹¤ì‹œê°„ ML ì„œë¹™
- [ ] ë¶„ì‚° ì²˜ë¦¬ ìµœì í™”
- [ ] ì»¤ìŠ¤í…€ ë¶„ì„ ë„êµ¬

### ğŸ’ í”Œë˜í‹°ë„˜ ë ˆë²¨
- [ ] ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ ì„¤ê³„
- [ ] ê³ ê¸‰ ML íŒŒì´í”„ë¼ì¸
- [ ] ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì • ì‹œìŠ¤í…œ
- [ ] ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ì¸¡ì •

---

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ (Additional Resources)

### í•„ë…ì„œ
- ğŸ“– "Streaming Systems" - Tyler Akidau
- ğŸ“– "Designing Data-Intensive Applications" - Martin Kleppmann
- ğŸ“– "The Data Warehouse Toolkit" - Ralph Kimball

### ì˜¨ë¼ì¸ ê°•ì˜
- ğŸ“ Coursera: Big Data Specialization
- ğŸ“ Udacity: Data Streaming Nanodegree
- ğŸ“ Fast.ai: Practical Deep Learning

### ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸
- ğŸ”§ Apache Kafka: ë¶„ì‚° ìŠ¤íŠ¸ë¦¬ë° í”Œë«í¼
- ğŸ”§ Apache Spark: ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬
- ğŸ”§ ClickHouse: ì‹¤ì‹œê°„ ë¶„ì„ DB
- ğŸ”§ Metabase: ì˜¤í”ˆì†ŒìŠ¤ BI ë„êµ¬

### ì‹¤ë¬´ ë„êµ¬
- ğŸ“Š Tableau: ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤
- ğŸ“Š Looker: ë°ì´í„° í”Œë«í¼
- ğŸ“Š Amplitude: ì œí’ˆ ë¶„ì„
- ğŸ“Š Mixpanel: ì‚¬ìš©ì í–‰ë™ ë¶„ì„

**ê³„ì†í•´ì„œ ë‹¤ë¥¸ ê³ ê¸‰ ë¬¸ì„œë“¤ë„ í•™ìŠµí•´ë³´ì„¸ìš”! ğŸš€**